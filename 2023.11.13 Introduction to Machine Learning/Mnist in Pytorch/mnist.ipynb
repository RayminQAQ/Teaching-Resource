{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "    from torch import save, load, nn\n",
    "    from torch.optim import Adam\n",
    "    from torch.utils.data import DataLoader \n",
    "\"\"\"\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor \n",
    "\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_trainset), len(mnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([60000, 28, 28]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainset.data.size(), mnist_trainset.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: 60000\n",
      "Labels batch shape: 60000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "INDEX = 1\n",
    "\n",
    "# Display image and label.\n",
    "train_features, train_labels = mnist_trainset.data, mnist_trainset.targets\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()[0]}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()[0]}\")\n",
    "\n",
    "img = train_features[INDEX].squeeze()\n",
    "label = train_labels[INDEX] \n",
    "plt.title(f'Label: {label}') # label: torch.Tensor\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28,10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (network): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
      "    (2): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(Model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tensorflow, you just need to do **model.fit** && **model.compile** and it's done.\n",
    "\n",
    "But in Pytorch, you need Dataloader to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def train(model : torch.nn.Module, dataset_train : torchvision.datasets.MNIST, \n",
    "          loss_fn : torch.nn, optimizer : torch.optim,\n",
    "          EPOCH, BATCHSIZE):\n",
    "\"\"\"\n",
    "\n",
    "def train(model, dataset_train, loss_fn, optimizer, EPOCH, BATCHSIZE):\n",
    "    # transfer package from \"torchvision.dataset\" into \"torch.utils.data.Dataloader\"\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCHSIZE, num_workers=1)\n",
    "    \n",
    "    # Take into training phase\n",
    "    model.train()\n",
    "    \n",
    "    # Starting training\n",
    "    for i in range(EPOCH):\n",
    "        for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "            # Set cuda\n",
    "            #if torch.cuda.is_available():\n",
    "            #    imgs = imgs.cuda()\n",
    "            #    labels = labels.cuda()\n",
    "            \n",
    "            # Get gradient from loss function(loss_fn)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Update gradient from loss(loss)\n",
    "            optimizer.zero_grad() # zero out exist gradient\n",
    "            loss.backward() # to backpropagate gradients of prediction loss.\n",
    "            optimizer.step()  # apply Gradient decent: to adjust model parameters.  \n",
    "            \n",
    "            # Print message\n",
    "            if(batch_idx+1)%1000 == 0: \n",
    "                print(f\"Training Epoch: {i} [{batch_idx*len(imgs)}/{len(train_loader.dataset)}({round((batch_idx*len(imgs)/len(train_loader.dataset) * 100.))}%)]     Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, mnist_testset, loss_fuc, BATCHSIZE):\n",
    "    # transfer package from \"torchvision.dataset\" into \"torch.utils.data.Dataloader\"\n",
    "    test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=BATCHSIZE, num_workers=1)\n",
    "    \n",
    "    # Set model to evaluation/testing phase\n",
    "    model.eval()\n",
    "    \n",
    "    # Accuracy and Test_Loss\n",
    "    Total_accurate  = 0\n",
    "    Total_sample = 0\n",
    "    Test_Loss = []\n",
    "    \n",
    "    # Testing\n",
    "    with torch.no_grad():\n",
    "        for datas in test_loader:\n",
    "            # Load data into imgs, labels\n",
    "            # Notice that imgs and labels are 'array'w\n",
    "            imgs, labels = datas\n",
    "            \n",
    "            # Prediction: Loss\n",
    "            output = model(imgs)\n",
    "            loss = loss_fuc(output, labels)\n",
    "            \n",
    "            # Add Loss into Test_Loss(array)\n",
    "            Test_Loss.append(loss)\n",
    "            print(f\"Loss: {loss}\")\n",
    "            \n",
    "            # Prediction: counting Accuracy\n",
    "            # Using: output && labels\n",
    "            output = torch.max(output, 1)[1]\n",
    "            Total_accurate += (output == labels).sum().item()\n",
    "            Total_sample += len(output)\n",
    "            \n",
    "    # Print Test Loss\n",
    "    [print(\"=\", end='') for _ in range(1, 10)]\n",
    "    print(f\"\\nThe average loss is: {sum(Test_Loss) / len(Test_Loss)}\")  \n",
    "    print(f\"The final loss is: {Test_Loss[len(Test_Loss)-1]}\")  \n",
    "\n",
    "    # Print Accuracy \n",
    "    print(f\"The final accuracy is: {Total_accurate/Total_sample * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epoch = 2\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# Loss function (eg: nn.CrossEntropyLoss, nn.L1Loss, nn.MSELoss)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (it's all in torch.optim)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mnist_trainset**, previously appear in \"loadings mnist data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\姚睿銘\\Desktop\\NTUAI course\\mnist\\Pytorch\\mnist.ipynb 儲存格 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/%E5%A7%9A%E7%9D%BF%E9%8A%98/Desktop/NTUAI%20course/mnist/Pytorch/mnist.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, mnist_trainset, loss_function, optimizer, epoch, batch_size)\n",
      "\u001b[1;32mc:\\Users\\姚睿銘\\Desktop\\NTUAI course\\mnist\\Pytorch\\mnist.ipynb 儲存格 21\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%E5%A7%9A%E7%9D%BF%E9%8A%98/Desktop/NTUAI%20course/mnist/Pytorch/mnist.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%E5%A7%9A%E7%9D%BF%E9%8A%98/Desktop/NTUAI%20course/mnist/Pytorch/mnist.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Get gradient from loss function(loss_fn)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/%E5%A7%9A%E7%9D%BF%E9%8A%98/Desktop/NTUAI%20course/mnist/Pytorch/mnist.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(imgs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%E5%A7%9A%E7%9D%BF%E9%8A%98/Desktop/NTUAI%20course/mnist/Pytorch/mnist.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%E5%A7%9A%E7%9D%BF%E9%8A%98/Desktop/NTUAI%20course/mnist/Pytorch/mnist.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Update gradient from loss(loss)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\姚睿銘\\Desktop\\NTUAI course\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\姚睿銘\\Desktop\\NTUAI course\\mnist\\Pytorch\\mnist.ipynb 儲存格 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%E5%A7%9A%E7%9D%BF%E9%8A%98/Desktop/NTUAI%20course/mnist/Pytorch/mnist.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/%E5%A7%9A%E7%9D%BF%E9%8A%98/Desktop/NTUAI%20course/mnist/Pytorch/mnist.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(x)\n",
      "File \u001b[1;32mc:\\Users\\姚睿銘\\Desktop\\NTUAI course\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\姚睿銘\\Desktop\\NTUAI course\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\姚睿銘\\Desktop\\NTUAI course\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\姚睿銘\\Desktop\\NTUAI course\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "train(model, mnist_trainset, loss_function, optimizer, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4612964391708374\n",
      "Loss: 1.4624863862991333\n",
      "Loss: 1.7121855020523071\n",
      "Loss: 1.4625022411346436\n",
      "Loss: 1.4627732038497925\n",
      "Loss: 1.4668546915054321\n",
      "Loss: 1.4634989500045776\n",
      "Loss: 1.461591362953186\n",
      "Loss: 1.6719794273376465\n",
      "Loss: 1.5945279598236084\n",
      "Loss: 1.4704513549804688\n",
      "Loss: 1.4775400161743164\n",
      "Loss: 1.4619903564453125\n",
      "Loss: 1.4643583297729492\n",
      "Loss: 1.4793652296066284\n",
      "Loss: 1.5773952007293701\n",
      "Loss: 1.5823439359664917\n",
      "Loss: 1.4611525535583496\n",
      "Loss: 1.4720144271850586\n",
      "Loss: 1.4951889514923096\n",
      "Loss: 1.5416643619537354\n",
      "Loss: 1.513486623764038\n",
      "Loss: 1.4611716270446777\n",
      "Loss: 1.5276825428009033\n",
      "Loss: 1.46234130859375\n",
      "Loss: 1.4612174034118652\n",
      "Loss: 1.4659587144851685\n",
      "Loss: 1.6510435342788696\n",
      "Loss: 1.4779996871948242\n",
      "Loss: 1.6590452194213867\n",
      "Loss: 1.5143799781799316\n",
      "Loss: 1.725819706916809\n",
      "Loss: 1.4616062641143799\n",
      "Loss: 1.4632844924926758\n",
      "Loss: 1.5589317083358765\n",
      "Loss: 1.4615267515182495\n",
      "Loss: 1.5997447967529297\n",
      "Loss: 1.9325664043426514\n",
      "Loss: 1.471205234527588\n",
      "Loss: 1.499415636062622\n",
      "Loss: 1.461157202720642\n",
      "Loss: 1.465890884399414\n",
      "Loss: 1.4651262760162354\n",
      "Loss: 1.5072561502456665\n",
      "Loss: 1.469316840171814\n",
      "Loss: 1.4641667604446411\n",
      "Loss: 1.480691909790039\n",
      "Loss: 1.4653186798095703\n",
      "Loss: 1.8612661361694336\n",
      "Loss: 1.465093970298767\n",
      "Loss: 1.4619638919830322\n",
      "Loss: 1.4615949392318726\n",
      "Loss: 1.5908935070037842\n",
      "Loss: 1.4630898237228394\n",
      "Loss: 1.7275209426879883\n",
      "Loss: 1.4627678394317627\n",
      "Loss: 1.4692327976226807\n",
      "Loss: 1.465070128440857\n",
      "Loss: 1.7004666328430176\n",
      "Loss: 1.4614640474319458\n",
      "Loss: 1.7597230672836304\n",
      "Loss: 1.7186975479125977\n",
      "Loss: 1.4702072143554688\n",
      "Loss: 1.4647908210754395\n",
      "Loss: 1.809962272644043\n",
      "Loss: 1.463310718536377\n",
      "Loss: 1.5835930109024048\n",
      "Loss: 1.4665091037750244\n",
      "Loss: 1.4823079109191895\n",
      "Loss: 1.4620810747146606\n",
      "Loss: 1.4851739406585693\n",
      "Loss: 1.4612796306610107\n",
      "Loss: 1.7110540866851807\n",
      "Loss: 1.4626126289367676\n",
      "Loss: 1.4803935289382935\n",
      "Loss: 1.634690761566162\n",
      "Loss: 1.6861687898635864\n",
      "Loss: 1.4613430500030518\n",
      "Loss: 1.6227463483810425\n",
      "Loss: 1.7056889533996582\n",
      "Loss: 1.9519715309143066\n",
      "Loss: 1.6499783992767334\n",
      "Loss: 1.4620447158813477\n",
      "Loss: 1.4631719589233398\n",
      "Loss: 1.5593669414520264\n",
      "Loss: 1.7483043670654297\n",
      "Loss: 1.6180598735809326\n",
      "Loss: 1.464802861213684\n",
      "Loss: 1.715074062347412\n",
      "Loss: 1.5702407360076904\n",
      "Loss: 1.7063180208206177\n",
      "Loss: 1.4657398462295532\n",
      "Loss: 1.4616940021514893\n",
      "Loss: 1.4631311893463135\n",
      "Loss: 1.4821857213974\n",
      "Loss: 1.530557632446289\n",
      "Loss: 1.533829689025879\n",
      "Loss: 1.5224332809448242\n",
      "Loss: 1.462647795677185\n",
      "Loss: 1.4618148803710938\n",
      "Loss: 1.6263452768325806\n",
      "Loss: 1.6025397777557373\n",
      "Loss: 1.46262788772583\n",
      "Loss: 1.4745876789093018\n",
      "Loss: 1.4669945240020752\n",
      "Loss: 1.6404554843902588\n",
      "Loss: 1.461942434310913\n",
      "Loss: 1.4843013286590576\n",
      "Loss: 1.6817547082901\n",
      "Loss: 1.4794197082519531\n",
      "Loss: 1.4649337530136108\n",
      "Loss: 1.9842582941055298\n",
      "Loss: 1.957181453704834\n",
      "Loss: 1.4616923332214355\n",
      "Loss: 1.5841219425201416\n",
      "Loss: 1.4686756134033203\n",
      "Loss: 1.4981399774551392\n",
      "Loss: 1.711643099784851\n",
      "Loss: 1.462523341178894\n",
      "Loss: 1.8567843437194824\n",
      "Loss: 1.4817399978637695\n",
      "Loss: 1.4685704708099365\n",
      "Loss: 1.4770681858062744\n",
      "Loss: 1.6970778703689575\n",
      "Loss: 1.4659273624420166\n",
      "Loss: 1.6088435649871826\n",
      "Loss: 1.7082839012145996\n",
      "Loss: 1.7380752563476562\n",
      "Loss: 1.4975448846817017\n",
      "Loss: 1.4823054075241089\n",
      "Loss: 1.4676750898361206\n",
      "Loss: 1.481092929840088\n",
      "Loss: 1.775826334953308\n",
      "Loss: 1.4886536598205566\n",
      "Loss: 1.4687385559082031\n",
      "Loss: 1.7967579364776611\n",
      "Loss: 1.487353801727295\n",
      "Loss: 1.8540198802947998\n",
      "Loss: 1.5077346563339233\n",
      "Loss: 1.4614002704620361\n",
      "Loss: 1.4645309448242188\n",
      "Loss: 1.707618236541748\n",
      "Loss: 1.8178486824035645\n",
      "Loss: 1.5027269124984741\n",
      "Loss: 1.70402991771698\n",
      "Loss: 1.6799050569534302\n",
      "Loss: 1.4613311290740967\n",
      "Loss: 1.7081999778747559\n",
      "Loss: 1.4765733480453491\n",
      "Loss: 1.4667868614196777\n",
      "Loss: 1.4712879657745361\n",
      "Loss: 1.7523741722106934\n",
      "Loss: 1.6679199934005737\n",
      "Loss: 1.6792511940002441\n",
      "Loss: 1.689953088760376\n",
      "Loss: 1.4626727104187012\n",
      "Loss: 1.7865784168243408\n",
      "Loss: 1.947599172592163\n",
      "Loss: 1.4617570638656616\n",
      "Loss: 1.468851923942566\n",
      "Loss: 1.4871275424957275\n",
      "Loss: 1.4640448093414307\n",
      "Loss: 1.4622290134429932\n",
      "Loss: 1.4738447666168213\n",
      "Loss: 1.9416348934173584\n",
      "Loss: 1.4625252485275269\n",
      "Loss: 1.5125669240951538\n",
      "Loss: 1.461741328239441\n",
      "Loss: 1.4621784687042236\n",
      "Loss: 1.4623538255691528\n",
      "Loss: 1.4624648094177246\n",
      "Loss: 1.695890188217163\n",
      "Loss: 1.878018856048584\n",
      "Loss: 1.51259446144104\n",
      "Loss: 1.5020382404327393\n",
      "Loss: 1.4621000289916992\n",
      "Loss: 1.7095481157302856\n",
      "Loss: 1.4612343311309814\n",
      "Loss: 1.5877244472503662\n",
      "Loss: 1.7122297286987305\n",
      "Loss: 1.703202247619629\n",
      "Loss: 1.473068356513977\n",
      "Loss: 1.5609302520751953\n",
      "Loss: 1.5133029222488403\n",
      "Loss: 1.5827946662902832\n",
      "Loss: 1.9251405000686646\n",
      "Loss: 1.4617540836334229\n",
      "Loss: 1.4705182313919067\n",
      "Loss: 1.4634015560150146\n",
      "Loss: 1.4652187824249268\n",
      "Loss: 1.6795406341552734\n",
      "Loss: 1.4615613222122192\n",
      "Loss: 1.4623521566390991\n",
      "Loss: 1.4756563901901245\n",
      "Loss: 1.4611849784851074\n",
      "Loss: 1.6111903190612793\n",
      "Loss: 1.5213598012924194\n",
      "Loss: 1.7005547285079956\n",
      "Loss: 1.4891091585159302\n",
      "Loss: 1.4757187366485596\n",
      "Loss: 1.5892688035964966\n",
      "Loss: 1.4695196151733398\n",
      "Loss: 1.4646308422088623\n",
      "Loss: 1.461350440979004\n",
      "Loss: 1.4624629020690918\n",
      "Loss: 1.4821174144744873\n",
      "Loss: 1.4973773956298828\n",
      "Loss: 1.5489370822906494\n",
      "Loss: 1.4752002954483032\n",
      "Loss: 1.7117412090301514\n",
      "Loss: 1.4699347019195557\n",
      "Loss: 1.7652851343154907\n",
      "Loss: 1.4612361192703247\n",
      "Loss: 1.4660043716430664\n",
      "Loss: 1.5432754755020142\n",
      "Loss: 1.4619799852371216\n",
      "Loss: 1.4788762331008911\n",
      "Loss: 1.4679415225982666\n",
      "Loss: 1.4755196571350098\n",
      "Loss: 1.7411317825317383\n",
      "Loss: 1.6671922206878662\n",
      "Loss: 1.4719654321670532\n",
      "Loss: 1.6026268005371094\n",
      "Loss: 1.4630067348480225\n",
      "Loss: 1.7017695903778076\n",
      "Loss: 1.6032785177230835\n",
      "Loss: 1.4665660858154297\n",
      "Loss: 1.463453769683838\n",
      "Loss: 1.4793602228164673\n",
      "Loss: 1.4618135690689087\n",
      "Loss: 1.4647722244262695\n",
      "Loss: 1.7606687545776367\n",
      "Loss: 1.6430332660675049\n",
      "Loss: 1.4624696969985962\n",
      "Loss: 1.9712392091751099\n",
      "Loss: 1.4614795446395874\n",
      "Loss: 1.8504531383514404\n",
      "Loss: 1.73966646194458\n",
      "Loss: 1.4638954401016235\n",
      "Loss: 1.8549423217773438\n",
      "Loss: 1.6142548322677612\n",
      "Loss: 1.7118091583251953\n",
      "Loss: 1.486120343208313\n",
      "Loss: 1.6702324151992798\n",
      "Loss: 1.4620442390441895\n",
      "Loss: 1.6802197694778442\n",
      "Loss: 1.4623377323150635\n",
      "Loss: 1.475855827331543\n",
      "Loss: 1.5883359909057617\n",
      "Loss: 1.5614547729492188\n",
      "Loss: 1.4674152135849\n",
      "Loss: 1.4693562984466553\n",
      "Loss: 1.461783766746521\n",
      "Loss: 1.9418203830718994\n",
      "Loss: 1.4715763330459595\n",
      "Loss: 1.4621386528015137\n",
      "Loss: 1.516542911529541\n",
      "Loss: 1.6236357688903809\n",
      "Loss: 1.7685496807098389\n",
      "Loss: 1.7078158855438232\n",
      "Loss: 1.4632883071899414\n",
      "Loss: 1.6135163307189941\n",
      "Loss: 1.6357474327087402\n",
      "Loss: 1.5763906240463257\n",
      "Loss: 1.4701564311981201\n",
      "Loss: 1.7046711444854736\n",
      "Loss: 1.4612977504730225\n",
      "Loss: 1.674328327178955\n",
      "Loss: 1.7122917175292969\n",
      "Loss: 1.46511709690094\n",
      "Loss: 1.6463496685028076\n",
      "Loss: 1.467179298400879\n",
      "Loss: 1.4762072563171387\n",
      "Loss: 1.5205426216125488\n",
      "Loss: 1.6387646198272705\n",
      "Loss: 1.7012178897857666\n",
      "Loss: 1.7112839221954346\n",
      "Loss: 1.472791075706482\n",
      "Loss: 1.9171826839447021\n",
      "Loss: 1.731383204460144\n",
      "Loss: 1.4653526544570923\n",
      "Loss: 1.592871904373169\n",
      "Loss: 1.4862372875213623\n",
      "Loss: 1.472525954246521\n",
      "Loss: 1.4622423648834229\n",
      "Loss: 1.4644356966018677\n",
      "Loss: 1.4645990133285522\n",
      "Loss: 1.4654806852340698\n",
      "Loss: 1.6379598379135132\n",
      "Loss: 1.556962013244629\n",
      "Loss: 1.461820363998413\n",
      "Loss: 1.5019019842147827\n",
      "Loss: 1.531715989112854\n",
      "Loss: 1.633937954902649\n",
      "Loss: 1.4630851745605469\n",
      "Loss: 1.745280146598816\n",
      "Loss: 1.5051617622375488\n",
      "Loss: 1.651267170906067\n",
      "Loss: 1.955631136894226\n",
      "Loss: 1.7111414670944214\n",
      "Loss: 1.8163491487503052\n",
      "Loss: 1.959534764289856\n",
      "Loss: 1.681351900100708\n",
      "Loss: 1.468509554862976\n",
      "Loss: 1.6189807653427124\n",
      "Loss: 1.4624292850494385\n",
      "Loss: 1.7579107284545898\n",
      "Loss: 1.6470199823379517\n",
      "Loss: 1.9566657543182373\n",
      "Loss: 1.4637941122055054\n",
      "Loss: 1.7296273708343506\n",
      "Loss: 1.7098270654678345\n",
      "Loss: 1.6840530633926392\n",
      "Loss: 1.5092334747314453\n",
      "Loss: 1.7097158432006836\n",
      "Loss: 1.7107232809066772\n",
      "Loss: 1.5191761255264282\n",
      "Loss: 1.5843303203582764\n",
      "Loss: 1.52326500415802\n",
      "Loss: 1.4847079515457153\n",
      "Loss: 1.739125370979309\n",
      "Loss: 1.496748924255371\n",
      "Loss: 1.694483995437622\n",
      "Loss: 1.463666558265686\n",
      "Loss: 1.717468023300171\n",
      "Loss: 1.4629042148590088\n",
      "Loss: 1.4613114595413208\n",
      "Loss: 1.5617719888687134\n",
      "Loss: 1.5835402011871338\n",
      "Loss: 1.7145237922668457\n",
      "Loss: 1.5742063522338867\n",
      "Loss: 1.9461756944656372\n",
      "Loss: 1.6768134832382202\n",
      "Loss: 1.4612983465194702\n",
      "Loss: 1.7327730655670166\n",
      "Loss: 1.4631109237670898\n",
      "Loss: 1.8606455326080322\n",
      "Loss: 1.4614766836166382\n",
      "Loss: 1.4744937419891357\n",
      "Loss: 1.5145974159240723\n",
      "Loss: 1.4613382816314697\n",
      "Loss: 1.6460233926773071\n",
      "Loss: 1.4655463695526123\n",
      "Loss: 1.504765272140503\n",
      "Loss: 1.5114758014678955\n",
      "Loss: 1.4662262201309204\n",
      "Loss: 1.4640952348709106\n",
      "Loss: 1.463752031326294\n",
      "Loss: 1.7338440418243408\n",
      "Loss: 1.465397834777832\n",
      "Loss: 1.6194490194320679\n",
      "Loss: 1.4725459814071655\n",
      "Loss: 1.849526047706604\n",
      "Loss: 1.7134771347045898\n",
      "Loss: 1.4611899852752686\n",
      "Loss: 1.500194787979126\n",
      "Loss: 1.4742305278778076\n",
      "Loss: 1.6373023986816406\n",
      "Loss: 1.7508598566055298\n",
      "Loss: 1.4794809818267822\n",
      "Loss: 1.6872355937957764\n",
      "Loss: 1.7314727306365967\n",
      "Loss: 1.461885690689087\n",
      "Loss: 1.5212979316711426\n",
      "Loss: 1.461417317390442\n",
      "Loss: 1.4782676696777344\n",
      "Loss: 1.9091167449951172\n",
      "Loss: 1.515015959739685\n",
      "Loss: 1.4798437356948853\n",
      "Loss: 1.4927568435668945\n",
      "Loss: 1.4623795747756958\n",
      "Loss: 1.4616438150405884\n",
      "Loss: 1.4615124464035034\n",
      "Loss: 1.6922093629837036\n",
      "Loss: 1.4726572036743164\n",
      "Loss: 1.7488099336624146\n",
      "Loss: 1.465129017829895\n",
      "Loss: 1.4614657163619995\n",
      "Loss: 1.5530688762664795\n",
      "Loss: 1.4682466983795166\n",
      "Loss: 1.7201992273330688\n",
      "Loss: 1.960429072380066\n",
      "Loss: 1.7332172393798828\n",
      "Loss: 1.4617210626602173\n",
      "Loss: 1.4640880823135376\n",
      "Loss: 1.4679940938949585\n",
      "Loss: 1.4760234355926514\n",
      "Loss: 1.7113754749298096\n",
      "Loss: 1.7397323846817017\n",
      "Loss: 1.7200746536254883\n",
      "Loss: 1.481518268585205\n",
      "Loss: 1.4625908136367798\n",
      "Loss: 1.789556622505188\n",
      "Loss: 1.4967375993728638\n",
      "Loss: 1.4769365787506104\n",
      "Loss: 1.7178844213485718\n",
      "Loss: 1.6974714994430542\n",
      "Loss: 1.461623191833496\n",
      "Loss: 1.487744927406311\n",
      "Loss: 1.4858888387680054\n",
      "Loss: 1.5203588008880615\n",
      "Loss: 1.6356189250946045\n",
      "Loss: 1.7308822870254517\n",
      "Loss: 1.4656457901000977\n",
      "Loss: 1.46917724609375\n",
      "Loss: 1.5452359914779663\n",
      "Loss: 1.6392439603805542\n",
      "Loss: 1.4618194103240967\n",
      "Loss: 1.6888470649719238\n",
      "Loss: 1.4713088274002075\n",
      "Loss: 1.890228509902954\n",
      "Loss: 1.550704002380371\n",
      "Loss: 1.4634950160980225\n",
      "Loss: 1.4683727025985718\n",
      "Loss: 1.4616807699203491\n",
      "Loss: 1.56343412399292\n",
      "Loss: 1.4757574796676636\n",
      "Loss: 1.5058441162109375\n",
      "Loss: 1.4625108242034912\n",
      "Loss: 1.6792056560516357\n",
      "Loss: 1.7109766006469727\n",
      "Loss: 1.7331925630569458\n",
      "Loss: 1.462972640991211\n",
      "Loss: 1.5403556823730469\n",
      "Loss: 1.698847770690918\n",
      "Loss: 1.5323638916015625\n",
      "Loss: 1.4636776447296143\n",
      "Loss: 1.7063411474227905\n",
      "Loss: 1.4619362354278564\n",
      "Loss: 1.9828331470489502\n",
      "Loss: 1.7905850410461426\n",
      "Loss: 1.519776463508606\n",
      "Loss: 1.491297721862793\n",
      "Loss: 1.6745502948760986\n",
      "Loss: 1.4884933233261108\n",
      "Loss: 1.573266625404358\n",
      "Loss: 1.4626421928405762\n",
      "Loss: 1.6703662872314453\n",
      "Loss: 1.711942195892334\n",
      "Loss: 1.5006896257400513\n",
      "Loss: 1.4682683944702148\n",
      "Loss: 1.6292610168457031\n",
      "Loss: 1.4621918201446533\n",
      "Loss: 1.963066816329956\n",
      "Loss: 1.4618232250213623\n",
      "Loss: 1.473966360092163\n",
      "Loss: 1.4709336757659912\n",
      "Loss: 1.7067809104919434\n",
      "Loss: 1.4691237211227417\n",
      "Loss: 1.4891364574432373\n",
      "Loss: 1.698913812637329\n",
      "Loss: 1.4619709253311157\n",
      "Loss: 1.4961298704147339\n",
      "Loss: 1.7057569026947021\n",
      "Loss: 1.555217981338501\n",
      "Loss: 1.5555285215377808\n",
      "Loss: 1.4635287523269653\n",
      "Loss: 1.7032699584960938\n",
      "Loss: 1.501802921295166\n",
      "Loss: 1.6669930219650269\n",
      "Loss: 1.4808509349822998\n",
      "Loss: 1.4684922695159912\n",
      "Loss: 1.707615613937378\n",
      "Loss: 1.5621960163116455\n",
      "Loss: 1.561322569847107\n",
      "Loss: 1.4731398820877075\n",
      "Loss: 1.6954712867736816\n",
      "Loss: 1.5938090085983276\n",
      "Loss: 1.4692411422729492\n",
      "Loss: 1.7234373092651367\n",
      "Loss: 1.7730010747909546\n",
      "Loss: 1.4636911153793335\n",
      "Loss: 1.4991707801818848\n",
      "Loss: 1.4673609733581543\n",
      "Loss: 1.6207377910614014\n",
      "Loss: 1.8628826141357422\n",
      "Loss: 1.4633138179779053\n",
      "Loss: 1.5055603981018066\n",
      "Loss: 1.5022703409194946\n",
      "Loss: 1.6696281433105469\n",
      "Loss: 1.4876935482025146\n",
      "Loss: 1.5807347297668457\n",
      "Loss: 1.6766984462738037\n",
      "Loss: 1.463009238243103\n",
      "Loss: 1.7036927938461304\n",
      "Loss: 1.8181672096252441\n",
      "Loss: 1.4614217281341553\n",
      "Loss: 1.4805489778518677\n",
      "Loss: 1.9367876052856445\n",
      "Loss: 1.543891429901123\n",
      "Loss: 1.5340569019317627\n",
      "Loss: 1.4650894403457642\n",
      "Loss: 1.8610974550247192\n",
      "Loss: 1.7094550132751465\n",
      "Loss: 1.4678786993026733\n",
      "Loss: 1.9045040607452393\n",
      "Loss: 1.7085354328155518\n",
      "Loss: 1.5283617973327637\n",
      "Loss: 1.5477006435394287\n",
      "Loss: 1.462249755859375\n",
      "Loss: 1.521657943725586\n",
      "Loss: 1.6929616928100586\n",
      "Loss: 1.466010570526123\n",
      "Loss: 1.4615131616592407\n",
      "Loss: 1.7114183902740479\n",
      "Loss: 1.4630149602890015\n",
      "Loss: 1.7105605602264404\n",
      "Loss: 1.4725568294525146\n",
      "Loss: 1.581026315689087\n",
      "Loss: 1.537721037864685\n",
      "Loss: 1.9568952322006226\n",
      "Loss: 1.7256653308868408\n",
      "Loss: 1.5021872520446777\n",
      "Loss: 1.7247967720031738\n",
      "Loss: 1.473508358001709\n",
      "Loss: 1.5021718740463257\n",
      "Loss: 1.4765279293060303\n",
      "Loss: 1.7886418104171753\n",
      "Loss: 1.5627251863479614\n",
      "Loss: 1.463399887084961\n",
      "Loss: 1.462321400642395\n",
      "Loss: 1.4619333744049072\n",
      "Loss: 1.4730583429336548\n",
      "Loss: 1.579524040222168\n",
      "Loss: 1.8691754341125488\n",
      "Loss: 1.461907982826233\n",
      "Loss: 1.7452954053878784\n",
      "Loss: 1.7485785484313965\n",
      "Loss: 1.669421672821045\n",
      "Loss: 1.7236371040344238\n",
      "Loss: 1.4947116374969482\n",
      "Loss: 1.5525474548339844\n",
      "Loss: 1.9628806114196777\n",
      "Loss: 1.735970377922058\n",
      "Loss: 1.6277992725372314\n",
      "Loss: 1.4619539976119995\n",
      "Loss: 1.4732892513275146\n",
      "Loss: 1.6703641414642334\n",
      "Loss: 1.5805535316467285\n",
      "Loss: 1.4613571166992188\n",
      "Loss: 1.4773415327072144\n",
      "Loss: 1.4619420766830444\n",
      "Loss: 1.4993284940719604\n",
      "Loss: 1.5720208883285522\n",
      "Loss: 1.539527177810669\n",
      "Loss: 1.7696937322616577\n",
      "Loss: 1.9455071687698364\n",
      "Loss: 1.7105870246887207\n",
      "Loss: 1.4945982694625854\n",
      "Loss: 1.4951409101486206\n",
      "Loss: 1.5525288581848145\n",
      "Loss: 1.4809362888336182\n",
      "Loss: 1.5979235172271729\n",
      "Loss: 1.655485987663269\n",
      "Loss: 1.4904569387435913\n",
      "Loss: 1.4666064977645874\n",
      "Loss: 1.7091279029846191\n",
      "Loss: 1.4620863199234009\n",
      "Loss: 1.5184670686721802\n",
      "Loss: 1.4617706537246704\n",
      "Loss: 1.4617280960083008\n",
      "Loss: 1.4757407903671265\n",
      "Loss: 1.462489366531372\n",
      "Loss: 1.4641069173812866\n",
      "Loss: 1.46493399143219\n",
      "Loss: 1.4628063440322876\n",
      "Loss: 1.7163958549499512\n",
      "Loss: 1.4975378513336182\n",
      "Loss: 1.714234471321106\n",
      "Loss: 1.4722342491149902\n",
      "Loss: 1.4734834432601929\n",
      "Loss: 1.4644968509674072\n",
      "Loss: 1.6669784784317017\n",
      "Loss: 1.705161213874817\n",
      "Loss: 1.7742133140563965\n",
      "Loss: 1.4630966186523438\n",
      "Loss: 1.4916996955871582\n",
      "Loss: 1.5083248615264893\n",
      "Loss: 1.497328519821167\n",
      "Loss: 1.4650872945785522\n",
      "Loss: 1.481489658355713\n",
      "Loss: 1.6883654594421387\n",
      "Loss: 1.4680038690567017\n",
      "Loss: 1.4782350063323975\n",
      "Loss: 1.4658764600753784\n",
      "Loss: 1.506164789199829\n",
      "Loss: 1.46378755569458\n",
      "Loss: 1.4658265113830566\n",
      "Loss: 1.461160659790039\n",
      "Loss: 1.5076206922531128\n",
      "Loss: 1.550524115562439\n",
      "Loss: 1.499743103981018\n",
      "Loss: 1.9310534000396729\n",
      "Loss: 1.5312010049819946\n",
      "Loss: 1.4998434782028198\n",
      "Loss: 1.847764015197754\n",
      "Loss: 1.725014567375183\n",
      "Loss: 1.468302607536316\n",
      "Loss: 1.9677313566207886\n",
      "Loss: 1.485802173614502\n",
      "Loss: 1.4656662940979004\n",
      "Loss: 1.786332368850708\n",
      "Loss: 1.6648274660110474\n",
      "Loss: 1.547516107559204\n",
      "Loss: 1.4684909582138062\n",
      "Loss: 1.7382516860961914\n",
      "Loss: 1.697716474533081\n",
      "Loss: 1.5063849687576294\n",
      "Loss: 1.7086433172225952\n",
      "Loss: 1.465135097503662\n",
      "Loss: 1.462142825126648\n",
      "Loss: 1.5565111637115479\n",
      "Loss: 1.6767786741256714\n",
      "Loss: 1.4735562801361084\n",
      "Loss: 1.4692838191986084\n",
      "Loss: 1.764407753944397\n",
      "Loss: 1.4680821895599365\n",
      "Loss: 1.4619741439819336\n",
      "Loss: 1.5044167041778564\n",
      "Loss: 1.4667482376098633\n",
      "Loss: 1.4757752418518066\n",
      "Loss: 1.4612246751785278\n",
      "Loss: 1.7013890743255615\n",
      "Loss: 1.4619848728179932\n",
      "Loss: 1.4729846715927124\n",
      "Loss: 1.4636881351470947\n",
      "Loss: 1.6569595336914062\n",
      "Loss: 1.497450351715088\n",
      "Loss: 1.673317790031433\n",
      "Loss: 1.4780092239379883\n",
      "Loss: 1.6511850357055664\n",
      "Loss: 1.4995381832122803\n",
      "Loss: 1.4814242124557495\n",
      "Loss: 1.6611839532852173\n",
      "Loss: 1.5747005939483643\n",
      "Loss: 1.550248384475708\n",
      "Loss: 1.4864702224731445\n",
      "Loss: 1.4882816076278687\n",
      "Loss: 1.476374626159668\n",
      "Loss: 1.6752313375473022\n",
      "Loss: 1.6545288562774658\n",
      "Loss: 1.4968440532684326\n",
      "Loss: 1.4723634719848633\n",
      "Loss: 1.824476718902588\n",
      "Loss: 1.692841649055481\n",
      "Loss: 1.4732027053833008\n",
      "Loss: 1.5522891283035278\n",
      "Loss: 1.47062087059021\n",
      "Loss: 1.5255833864212036\n",
      "Loss: 1.7056488990783691\n",
      "Loss: 1.4622323513031006\n",
      "Loss: 1.7505826950073242\n",
      "Loss: 1.7688281536102295\n",
      "Loss: 1.4614300727844238\n",
      "Loss: 1.4909191131591797\n",
      "Loss: 1.4630199670791626\n",
      "Loss: 1.4631215333938599\n",
      "Loss: 1.8706952333450317\n",
      "Loss: 1.700519323348999\n",
      "Loss: 1.5222938060760498\n",
      "Loss: 1.5099077224731445\n",
      "Loss: 1.5476018190383911\n",
      "Loss: 1.8318413496017456\n",
      "Loss: 1.7117762565612793\n",
      "Loss: 1.4854787588119507\n",
      "Loss: 1.4613237380981445\n",
      "Loss: 1.465055227279663\n",
      "Loss: 1.9335143566131592\n",
      "Loss: 1.4632163047790527\n",
      "Loss: 1.462423324584961\n",
      "Loss: 1.5280882120132446\n",
      "Loss: 1.4627933502197266\n",
      "Loss: 1.4845538139343262\n",
      "Loss: 1.7238926887512207\n",
      "Loss: 1.585423231124878\n",
      "Loss: 1.4721803665161133\n",
      "Loss: 1.4631850719451904\n",
      "Loss: 1.4876461029052734\n",
      "Loss: 1.5551342964172363\n",
      "Loss: 1.4625362157821655\n",
      "Loss: 1.657750129699707\n",
      "Loss: 1.5693055391311646\n",
      "Loss: 1.6811647415161133\n",
      "Loss: 1.4667620658874512\n",
      "Loss: 1.4864391088485718\n",
      "Loss: 1.596742868423462\n",
      "Loss: 1.4703500270843506\n",
      "Loss: 1.7233636379241943\n",
      "Loss: 1.4765862226486206\n",
      "Loss: 1.6844823360443115\n",
      "Loss: 1.551571011543274\n",
      "Loss: 1.461495041847229\n",
      "Loss: 1.8077950477600098\n",
      "Loss: 1.4618637561798096\n",
      "Loss: 1.4770563840866089\n",
      "Loss: 1.7244958877563477\n",
      "Loss: 1.4616258144378662\n",
      "Loss: 1.4624072313308716\n",
      "Loss: 1.472339153289795\n",
      "Loss: 1.4619030952453613\n",
      "Loss: 1.4657173156738281\n",
      "Loss: 1.4717974662780762\n",
      "Loss: 1.6592460870742798\n",
      "Loss: 1.619117259979248\n",
      "Loss: 1.6546076536178589\n",
      "Loss: 1.5141063928604126\n",
      "Loss: 1.4624637365341187\n",
      "Loss: 1.4659501314163208\n",
      "Loss: 1.8291610479354858\n",
      "Loss: 1.4979850053787231\n",
      "Loss: 1.4613776206970215\n",
      "Loss: 1.4620133638381958\n",
      "Loss: 1.7767513990402222\n",
      "Loss: 1.6272521018981934\n",
      "Loss: 1.4647738933563232\n",
      "Loss: 1.6102455854415894\n",
      "Loss: 1.5854991674423218\n",
      "Loss: 1.4634078741073608\n",
      "Loss: 1.462681770324707\n",
      "Loss: 1.5475355386734009\n",
      "Loss: 1.4625210762023926\n",
      "Loss: 1.4659603834152222\n",
      "Loss: 1.4655389785766602\n",
      "Loss: 1.461252212524414\n",
      "Loss: 1.7128770351409912\n",
      "Loss: 1.471440315246582\n",
      "Loss: 1.9576948881149292\n",
      "Loss: 1.498813271522522\n",
      "Loss: 1.505634069442749\n",
      "Loss: 1.6520508527755737\n",
      "Loss: 1.6186437606811523\n",
      "Loss: 1.960688591003418\n",
      "Loss: 1.6378939151763916\n",
      "Loss: 1.4621440172195435\n",
      "Loss: 1.506311297416687\n",
      "Loss: 1.463918924331665\n",
      "Loss: 1.6561002731323242\n",
      "Loss: 1.462583065032959\n",
      "Loss: 1.7837319374084473\n",
      "Loss: 1.4650049209594727\n",
      "Loss: 1.46210515499115\n",
      "Loss: 1.4646720886230469\n",
      "Loss: 1.58767569065094\n",
      "Loss: 1.462119698524475\n",
      "Loss: 1.5016653537750244\n",
      "Loss: 1.4613162279129028\n",
      "Loss: 1.4862786531448364\n",
      "Loss: 1.6356395483016968\n",
      "Loss: 1.7234292030334473\n",
      "Loss: 1.4744579792022705\n",
      "Loss: 1.4701404571533203\n",
      "Loss: 1.6772164106369019\n",
      "Loss: 1.46332848072052\n",
      "Loss: 1.4710699319839478\n",
      "Loss: 1.4612703323364258\n",
      "Loss: 1.6033872365951538\n",
      "Loss: 1.461348295211792\n",
      "Loss: 1.5409033298492432\n",
      "Loss: 1.5034081935882568\n",
      "Loss: 1.4688913822174072\n",
      "Loss: 1.4640992879867554\n",
      "Loss: 1.4623299837112427\n",
      "Loss: 1.5120009183883667\n",
      "Loss: 1.4621387720108032\n",
      "Loss: 1.4611706733703613\n",
      "Loss: 1.8090555667877197\n",
      "Loss: 1.6061393022537231\n",
      "Loss: 1.4667881727218628\n",
      "Loss: 1.7030820846557617\n",
      "Loss: 1.488317847251892\n",
      "Loss: 1.4611740112304688\n",
      "Loss: 1.5170419216156006\n",
      "Loss: 1.4678049087524414\n",
      "Loss: 1.4739116430282593\n",
      "Loss: 1.524631381034851\n",
      "Loss: 1.672511100769043\n",
      "Loss: 1.6673264503479004\n",
      "Loss: 1.8669344186782837\n",
      "Loss: 1.6791350841522217\n",
      "Loss: 1.7105739116668701\n",
      "Loss: 1.465524435043335\n",
      "Loss: 1.4634687900543213\n",
      "Loss: 1.6959702968597412\n",
      "Loss: 1.862334966659546\n",
      "Loss: 1.7674663066864014\n",
      "Loss: 1.4612605571746826\n",
      "Loss: 1.7160367965698242\n",
      "Loss: 1.5600368976593018\n",
      "Loss: 1.4734218120574951\n",
      "Loss: 1.5091077089309692\n",
      "Loss: 1.575600266456604\n",
      "Loss: 1.7443211078643799\n",
      "Loss: 1.4718061685562134\n",
      "Loss: 1.4680691957473755\n",
      "Loss: 1.4662857055664062\n",
      "Loss: 1.4729667901992798\n",
      "Loss: 1.530647873878479\n",
      "Loss: 1.7088308334350586\n",
      "Loss: 1.5650897026062012\n",
      "Loss: 1.4835495948791504\n",
      "Loss: 1.4631134271621704\n",
      "Loss: 1.807544469833374\n",
      "Loss: 1.4672127962112427\n",
      "Loss: 1.4635950326919556\n",
      "Loss: 1.700762391090393\n",
      "Loss: 1.4629122018814087\n",
      "Loss: 1.5991920232772827\n",
      "Loss: 1.4613072872161865\n",
      "Loss: 1.4715560674667358\n",
      "Loss: 1.464164137840271\n",
      "Loss: 1.708871603012085\n",
      "Loss: 1.6365184783935547\n",
      "Loss: 1.46700119972229\n",
      "Loss: 1.6815948486328125\n",
      "Loss: 1.4616641998291016\n",
      "Loss: 1.6304303407669067\n",
      "Loss: 1.4612728357315063\n",
      "Loss: 1.6929512023925781\n",
      "Loss: 1.479854941368103\n",
      "Loss: 1.4628337621688843\n",
      "Loss: 1.6457037925720215\n",
      "Loss: 1.5578556060791016\n",
      "Loss: 1.8095405101776123\n",
      "Loss: 1.4665615558624268\n",
      "Loss: 1.4613139629364014\n",
      "Loss: 1.4764838218688965\n",
      "Loss: 1.463936686515808\n",
      "Loss: 1.461372971534729\n",
      "Loss: 1.4628188610076904\n",
      "Loss: 1.7906255722045898\n",
      "Loss: 1.6158980131149292\n",
      "Loss: 1.465671420097351\n",
      "Loss: 1.941361904144287\n",
      "Loss: 1.6997510194778442\n",
      "Loss: 1.5093660354614258\n",
      "Loss: 1.469370722770691\n",
      "Loss: 1.4953548908233643\n",
      "Loss: 1.4651341438293457\n",
      "Loss: 1.4611928462982178\n",
      "Loss: 1.4620742797851562\n",
      "Loss: 1.4625611305236816\n",
      "Loss: 1.462195634841919\n",
      "Loss: 1.656396508216858\n",
      "Loss: 1.461288571357727\n",
      "Loss: 1.5323834419250488\n",
      "Loss: 1.547429084777832\n",
      "Loss: 1.4675090312957764\n",
      "Loss: 1.5008331537246704\n",
      "Loss: 1.5932081937789917\n",
      "Loss: 1.4613584280014038\n",
      "Loss: 1.4611625671386719\n",
      "Loss: 1.67059326171875\n",
      "Loss: 1.470402717590332\n",
      "Loss: 1.468656063079834\n",
      "Loss: 1.4628742933273315\n",
      "Loss: 1.530225396156311\n",
      "Loss: 1.4643926620483398\n",
      "Loss: 1.5269556045532227\n",
      "Loss: 1.4692413806915283\n",
      "Loss: 1.6944971084594727\n",
      "Loss: 1.4703015089035034\n",
      "Loss: 1.471605658531189\n",
      "Loss: 1.470065712928772\n",
      "Loss: 1.461883544921875\n",
      "Loss: 1.4640220403671265\n",
      "Loss: 1.561707615852356\n",
      "Loss: 1.5043827295303345\n",
      "Loss: 1.5441664457321167\n",
      "Loss: 1.6799992322921753\n",
      "Loss: 1.4941614866256714\n",
      "Loss: 1.4613714218139648\n",
      "Loss: 1.4615049362182617\n",
      "Loss: 1.67433762550354\n",
      "Loss: 1.505791187286377\n",
      "Loss: 1.51316237449646\n",
      "Loss: 1.7545881271362305\n",
      "Loss: 1.4683942794799805\n",
      "Loss: 1.479215383529663\n",
      "Loss: 1.4726189374923706\n",
      "Loss: 1.617640733718872\n",
      "Loss: 1.7911078929901123\n",
      "Loss: 1.4718265533447266\n",
      "Loss: 1.4700937271118164\n",
      "Loss: 1.557287573814392\n",
      "Loss: 1.4645106792449951\n",
      "Loss: 1.4662656784057617\n",
      "Loss: 1.5154132843017578\n",
      "Loss: 1.9081885814666748\n",
      "Loss: 1.4680571556091309\n",
      "Loss: 1.8162673711776733\n",
      "Loss: 1.5370638370513916\n",
      "Loss: 1.7767863273620605\n",
      "Loss: 1.468271255493164\n",
      "Loss: 1.838421106338501\n",
      "Loss: 1.5090135335922241\n",
      "Loss: 1.695493459701538\n",
      "Loss: 1.4611871242523193\n",
      "Loss: 1.4621431827545166\n",
      "Loss: 1.464185118675232\n",
      "Loss: 1.9391335248947144\n",
      "Loss: 1.475843906402588\n",
      "Loss: 1.7130407094955444\n",
      "Loss: 1.4613037109375\n",
      "Loss: 1.5327327251434326\n",
      "Loss: 1.4627342224121094\n",
      "Loss: 1.4630800485610962\n",
      "Loss: 1.5491968393325806\n",
      "Loss: 1.710341453552246\n",
      "Loss: 1.4625422954559326\n",
      "Loss: 1.4653857946395874\n",
      "Loss: 1.4646943807601929\n",
      "Loss: 1.4682259559631348\n",
      "Loss: 1.4612200260162354\n",
      "Loss: 1.474139928817749\n",
      "Loss: 1.4633527994155884\n",
      "Loss: 1.704528570175171\n",
      "Loss: 1.5110294818878174\n",
      "Loss: 1.461262822151184\n",
      "Loss: 1.659694790840149\n",
      "Loss: 1.4616079330444336\n",
      "Loss: 1.6895898580551147\n",
      "Loss: 1.531111717224121\n",
      "Loss: 1.5569345951080322\n",
      "Loss: 1.4625074863433838\n",
      "Loss: 1.4873595237731934\n",
      "Loss: 1.5657840967178345\n",
      "Loss: 1.4802426099777222\n",
      "Loss: 1.4635660648345947\n",
      "Loss: 1.468176245689392\n",
      "Loss: 1.955586314201355\n",
      "Loss: 1.4620481729507446\n",
      "Loss: 1.8892312049865723\n",
      "Loss: 1.7074687480926514\n",
      "Loss: 1.6475107669830322\n",
      "Loss: 1.5265523195266724\n",
      "Loss: 1.4629254341125488\n",
      "Loss: 1.4613368511199951\n",
      "Loss: 1.911612629890442\n",
      "Loss: 1.633195400238037\n",
      "Loss: 1.7797510623931885\n",
      "Loss: 1.6918067932128906\n",
      "Loss: 1.7170155048370361\n",
      "Loss: 1.6881000995635986\n",
      "Loss: 1.4873080253601074\n",
      "Loss: 1.7140207290649414\n",
      "Loss: 1.7200758457183838\n",
      "Loss: 1.4672303199768066\n",
      "Loss: 1.4613076448440552\n",
      "Loss: 1.6323612928390503\n",
      "Loss: 1.708284616470337\n",
      "Loss: 1.7072112560272217\n",
      "Loss: 1.7094345092773438\n",
      "Loss: 1.959506630897522\n",
      "Loss: 1.4611927270889282\n",
      "Loss: 1.7085564136505127\n",
      "Loss: 1.9181363582611084\n",
      "Loss: 1.463249683380127\n",
      "Loss: 1.4656866788864136\n",
      "Loss: 1.6710448265075684\n",
      "Loss: 1.9720592498779297\n",
      "Loss: 1.4612534046173096\n",
      "Loss: 1.683541178703308\n",
      "Loss: 1.7399981021881104\n",
      "Loss: 1.9614875316619873\n",
      "Loss: 1.4633575677871704\n",
      "Loss: 1.6769375801086426\n",
      "Loss: 1.4614286422729492\n",
      "Loss: 1.706278920173645\n",
      "Loss: 1.4643394947052002\n",
      "Loss: 1.7120859622955322\n",
      "Loss: 1.468956708908081\n",
      "Loss: 1.4629462957382202\n",
      "Loss: 1.4615545272827148\n",
      "Loss: 1.710968017578125\n",
      "Loss: 1.461482048034668\n",
      "Loss: 1.7109657526016235\n",
      "Loss: 1.71268630027771\n",
      "Loss: 1.4614351987838745\n",
      "Loss: 1.46189546585083\n",
      "Loss: 1.4724688529968262\n",
      "Loss: 1.4623452425003052\n",
      "Loss: 1.7170970439910889\n",
      "Loss: 1.4797382354736328\n",
      "Loss: 1.4621107578277588\n",
      "Loss: 1.469249963760376\n",
      "Loss: 1.9604260921478271\n",
      "Loss: 1.7045016288757324\n",
      "Loss: 1.7091529369354248\n",
      "Loss: 1.6362783908843994\n",
      "Loss: 1.462740421295166\n",
      "Loss: 1.7474101781845093\n",
      "Loss: 1.5227024555206299\n",
      "Loss: 1.5113160610198975\n",
      "Loss: 1.4616796970367432\n",
      "Loss: 1.5569074153900146\n",
      "Loss: 1.4616740942001343\n",
      "Loss: 1.6737265586853027\n",
      "Loss: 1.603087306022644\n",
      "Loss: 1.5703108310699463\n",
      "Loss: 1.4719728231430054\n",
      "Loss: 1.8044296503067017\n",
      "Loss: 1.6448845863342285\n",
      "Loss: 1.4656018018722534\n",
      "Loss: 1.7091716527938843\n",
      "Loss: 1.7568269968032837\n",
      "Loss: 1.4611783027648926\n",
      "Loss: 1.493660807609558\n",
      "Loss: 1.4726895093917847\n",
      "Loss: 1.4612561464309692\n",
      "Loss: 1.4840484857559204\n",
      "Loss: 1.461604118347168\n",
      "Loss: 1.6195967197418213\n",
      "Loss: 1.4622149467468262\n",
      "Loss: 1.4717504978179932\n",
      "Loss: 1.5119552612304688\n",
      "Loss: 1.7064814567565918\n",
      "Loss: 1.69760000705719\n",
      "Loss: 1.5942656993865967\n",
      "Loss: 1.9261963367462158\n",
      "Loss: 1.9233392477035522\n",
      "Loss: 1.4771411418914795\n",
      "Loss: 1.4856560230255127\n",
      "Loss: 1.461854338645935\n",
      "Loss: 1.5972230434417725\n",
      "Loss: 1.4615967273712158\n",
      "Loss: 1.4833016395568848\n",
      "Loss: 1.4616914987564087\n",
      "Loss: 1.5114915370941162\n",
      "Loss: 1.461376667022705\n",
      "Loss: 1.5088942050933838\n",
      "Loss: 1.5130395889282227\n",
      "Loss: 1.4906001091003418\n",
      "Loss: 1.7056654691696167\n",
      "Loss: 1.4661468267440796\n",
      "Loss: 1.4732564687728882\n",
      "Loss: 1.7494473457336426\n",
      "Loss: 1.642324686050415\n",
      "Loss: 1.4612443447113037\n",
      "Loss: 1.9158706665039062\n",
      "Loss: 1.7388691902160645\n",
      "Loss: 1.6954103708267212\n",
      "Loss: 1.5183995962142944\n",
      "Loss: 1.461374044418335\n",
      "Loss: 1.554748296737671\n",
      "Loss: 1.9247902631759644\n",
      "Loss: 1.6500554084777832\n",
      "Loss: 1.5790789127349854\n",
      "Loss: 1.4622102975845337\n",
      "Loss: 1.4809057712554932\n",
      "Loss: 1.6409926414489746\n",
      "Loss: 1.6846011877059937\n",
      "Loss: 1.7478728294372559\n",
      "Loss: 1.713126301765442\n",
      "Loss: 1.7219568490982056\n",
      "Loss: 1.4627217054367065\n",
      "Loss: 1.4663766622543335\n",
      "Loss: 1.698143482208252\n",
      "Loss: 1.5615105628967285\n",
      "Loss: 1.4616336822509766\n",
      "Loss: 1.9078054428100586\n",
      "Loss: 1.4614425897598267\n",
      "Loss: 1.4758392572402954\n",
      "Loss: 1.6824748516082764\n",
      "Loss: 1.4799389839172363\n",
      "Loss: 1.6191242933273315\n",
      "Loss: 1.5151976346969604\n",
      "Loss: 1.6907672882080078\n",
      "Loss: 1.6181411743164062\n",
      "Loss: 1.4688935279846191\n",
      "Loss: 1.4623584747314453\n",
      "Loss: 1.4661152362823486\n",
      "Loss: 1.7024897336959839\n",
      "Loss: 1.73098886013031\n",
      "Loss: 1.5465203523635864\n",
      "Loss: 1.7061017751693726\n",
      "Loss: 1.9757410287857056\n",
      "Loss: 1.7113912105560303\n",
      "Loss: 1.5516362190246582\n",
      "Loss: 1.8239507675170898\n",
      "Loss: 1.6750192642211914\n",
      "Loss: 1.510823130607605\n",
      "Loss: 1.5024372339248657\n",
      "Loss: 1.5103189945220947\n",
      "Loss: 1.5049855709075928\n",
      "Loss: 1.4638433456420898\n",
      "Loss: 1.6234803199768066\n",
      "Loss: 1.561882495880127\n",
      "Loss: 1.4628863334655762\n",
      "Loss: 1.691986083984375\n",
      "Loss: 1.8193433284759521\n",
      "Loss: 1.475266933441162\n",
      "Loss: 1.4612617492675781\n",
      "Loss: 1.6285934448242188\n",
      "Loss: 1.7332850694656372\n",
      "Loss: 1.4987497329711914\n",
      "Loss: 1.7072153091430664\n",
      "Loss: 1.46389639377594\n",
      "Loss: 1.580743432044983\n",
      "Loss: 1.4613943099975586\n",
      "Loss: 1.4743167161941528\n",
      "Loss: 1.4914556741714478\n",
      "Loss: 1.5705652236938477\n",
      "Loss: 1.651968240737915\n",
      "Loss: 1.4662820100784302\n",
      "Loss: 1.473720908164978\n",
      "Loss: 1.7197656631469727\n",
      "Loss: 1.817661166191101\n",
      "Loss: 1.463956356048584\n",
      "Loss: 1.9395198822021484\n",
      "Loss: 1.5261473655700684\n",
      "Loss: 1.4839379787445068\n",
      "Loss: 1.4642890691757202\n",
      "Loss: 1.9020239114761353\n",
      "Loss: 1.634328842163086\n",
      "Loss: 1.462363362312317\n",
      "Loss: 1.5971189737319946\n",
      "Loss: 1.4612047672271729\n",
      "Loss: 1.4612059593200684\n",
      "Loss: 1.4627479314804077\n",
      "Loss: 1.6689788103103638\n",
      "Loss: 1.4984339475631714\n",
      "Loss: 1.463433861732483\n",
      "Loss: 1.4862861633300781\n",
      "Loss: 1.4656901359558105\n",
      "Loss: 1.9123200178146362\n",
      "Loss: 1.711743950843811\n",
      "Loss: 1.6797730922698975\n",
      "Loss: 1.4629217386245728\n",
      "Loss: 1.4660990238189697\n",
      "Loss: 1.480016827583313\n",
      "Loss: 1.7064026594161987\n",
      "Loss: 1.4611927270889282\n",
      "Loss: 1.4634685516357422\n",
      "Loss: 1.4616358280181885\n",
      "Loss: 1.6124536991119385\n",
      "Loss: 1.7098932266235352\n",
      "Loss: 1.4837183952331543\n",
      "Loss: 1.7240713834762573\n",
      "Loss: 1.4619150161743164\n",
      "Loss: 1.4622150659561157\n",
      "Loss: 1.4752757549285889\n",
      "Loss: 1.5743508338928223\n",
      "Loss: 1.7127728462219238\n",
      "Loss: 1.7165645360946655\n",
      "Loss: 1.577926754951477\n",
      "Loss: 1.710318684577942\n",
      "Loss: 1.4625895023345947\n",
      "Loss: 1.4627783298492432\n",
      "Loss: 1.4816721677780151\n",
      "Loss: 1.4647287130355835\n",
      "Loss: 1.7111902236938477\n",
      "Loss: 1.472090482711792\n",
      "Loss: 1.4668561220169067\n",
      "Loss: 1.7130646705627441\n",
      "Loss: 1.484737753868103\n",
      "Loss: 1.485451579093933\n",
      "Loss: 1.4665157794952393\n",
      "Loss: 1.5078344345092773\n",
      "Loss: 1.5818110704421997\n",
      "Loss: 1.7001515626907349\n",
      "Loss: 1.6149394512176514\n",
      "Loss: 1.4619808197021484\n",
      "Loss: 1.4637130498886108\n",
      "Loss: 1.4628703594207764\n",
      "Loss: 1.5402802228927612\n",
      "Loss: 1.467132806777954\n",
      "Loss: 1.4690515995025635\n",
      "Loss: 1.7089343070983887\n",
      "Loss: 1.47419273853302\n",
      "Loss: 1.4630887508392334\n",
      "Loss: 1.4630427360534668\n",
      "Loss: 1.4613466262817383\n",
      "Loss: 1.5194768905639648\n",
      "Loss: 1.6271106004714966\n",
      "Loss: 1.5466959476470947\n",
      "Loss: 1.576808214187622\n",
      "Loss: 1.4612400531768799\n",
      "Loss: 1.4690475463867188\n",
      "Loss: 1.4613463878631592\n",
      "Loss: 1.4628281593322754\n",
      "Loss: 1.7056001424789429\n",
      "Loss: 1.7551933526992798\n",
      "Loss: 1.7145538330078125\n",
      "Loss: 1.5330445766448975\n",
      "Loss: 1.5182511806488037\n",
      "Loss: 1.6876049041748047\n",
      "Loss: 1.468540906906128\n",
      "Loss: 1.7002406120300293\n",
      "Loss: 1.5253698825836182\n",
      "Loss: 1.4632630348205566\n",
      "Loss: 1.708570122718811\n",
      "Loss: 1.462037205696106\n",
      "Loss: 1.4620418548583984\n",
      "Loss: 1.4632530212402344\n",
      "Loss: 1.5006815195083618\n",
      "Loss: 1.4644567966461182\n",
      "Loss: 1.70902681350708\n",
      "Loss: 1.521044135093689\n",
      "Loss: 1.4622210264205933\n",
      "Loss: 1.5002232789993286\n",
      "Loss: 1.4618595838546753\n",
      "Loss: 1.702769160270691\n",
      "Loss: 1.7092667818069458\n",
      "Loss: 1.954298496246338\n",
      "Loss: 1.472540259361267\n",
      "Loss: 1.750154733657837\n",
      "Loss: 1.690682053565979\n",
      "Loss: 1.6197731494903564\n",
      "Loss: 1.4688904285430908\n",
      "Loss: 2.006924629211426\n",
      "Loss: 1.464500904083252\n",
      "Loss: 1.4656922817230225\n",
      "Loss: 1.4666492938995361\n",
      "Loss: 1.6846330165863037\n",
      "Loss: 1.4624732732772827\n",
      "Loss: 1.7865638732910156\n",
      "Loss: 1.4621974229812622\n",
      "Loss: 1.4652189016342163\n",
      "Loss: 1.7103379964828491\n",
      "Loss: 1.949385166168213\n",
      "Loss: 1.7064871788024902\n",
      "Loss: 1.7124215364456177\n",
      "Loss: 1.7277154922485352\n",
      "Loss: 1.5048496723175049\n",
      "Loss: 1.4775261878967285\n",
      "Loss: 1.4640417098999023\n",
      "Loss: 1.461276888847351\n",
      "Loss: 1.4761321544647217\n",
      "Loss: 1.6754268407821655\n",
      "Loss: 1.5043758153915405\n",
      "Loss: 1.489216923713684\n",
      "Loss: 1.4653600454330444\n",
      "Loss: 1.4877455234527588\n",
      "Loss: 1.4620980024337769\n",
      "Loss: 1.6002953052520752\n",
      "Loss: 1.7155088186264038\n",
      "Loss: 1.4614652395248413\n",
      "Loss: 1.7460139989852905\n",
      "Loss: 1.6808092594146729\n",
      "Loss: 1.7111835479736328\n",
      "Loss: 1.50106680393219\n",
      "Loss: 1.7723214626312256\n",
      "Loss: 1.468332290649414\n",
      "Loss: 1.466364860534668\n",
      "Loss: 1.6803661584854126\n",
      "Loss: 1.6392247676849365\n",
      "Loss: 1.4746530055999756\n",
      "Loss: 1.7098033428192139\n",
      "Loss: 1.5799508094787598\n",
      "Loss: 1.6605899333953857\n",
      "Loss: 1.660025954246521\n",
      "Loss: 1.461379885673523\n",
      "Loss: 1.4900734424591064\n",
      "Loss: 1.504246711730957\n",
      "Loss: 1.4622206687927246\n",
      "Loss: 1.465743064880371\n",
      "Loss: 1.462450385093689\n",
      "Loss: 1.4612467288970947\n",
      "Loss: 1.4611772298812866\n",
      "Loss: 1.6767065525054932\n",
      "Loss: 1.4629384279251099\n",
      "Loss: 1.7095386981964111\n",
      "Loss: 1.464442491531372\n",
      "Loss: 1.7112221717834473\n",
      "Loss: 1.4623548984527588\n",
      "Loss: 1.4693962335586548\n",
      "Loss: 1.9381599426269531\n",
      "Loss: 1.6971714496612549\n",
      "Loss: 1.5084500312805176\n",
      "Loss: 1.7083467245101929\n",
      "Loss: 1.463642954826355\n",
      "Loss: 1.5636951923370361\n",
      "Loss: 1.464829683303833\n",
      "Loss: 1.461669683456421\n",
      "Loss: 1.4630259275436401\n",
      "Loss: 1.477013349533081\n",
      "Loss: 1.4645028114318848\n",
      "Loss: 1.4623688459396362\n",
      "Loss: 1.4613628387451172\n",
      "Loss: 1.4642975330352783\n",
      "Loss: 1.462923526763916\n",
      "Loss: 1.4611529111862183\n",
      "Loss: 1.4728236198425293\n",
      "Loss: 1.5199215412139893\n",
      "Loss: 1.5010087490081787\n",
      "Loss: 1.9129916429519653\n",
      "Loss: 1.4638925790786743\n",
      "Loss: 1.4618711471557617\n",
      "Loss: 1.4614448547363281\n",
      "Loss: 1.5046820640563965\n",
      "Loss: 1.4622092247009277\n",
      "Loss: 1.6386313438415527\n",
      "Loss: 1.4633889198303223\n",
      "Loss: 1.4702799320220947\n",
      "Loss: 1.7566685676574707\n",
      "Loss: 1.6810635328292847\n",
      "Loss: 1.4698288440704346\n",
      "Loss: 1.4631643295288086\n",
      "Loss: 1.4730749130249023\n",
      "Loss: 1.461478590965271\n",
      "Loss: 1.6028709411621094\n",
      "Loss: 1.4720991849899292\n",
      "Loss: 1.7043390274047852\n",
      "Loss: 1.461294412612915\n",
      "Loss: 1.645561933517456\n",
      "Loss: 1.4617117643356323\n",
      "Loss: 1.4620126485824585\n",
      "Loss: 1.483712911605835\n",
      "Loss: 1.461505651473999\n",
      "Loss: 1.4823038578033447\n",
      "Loss: 1.4619834423065186\n",
      "Loss: 1.7118198871612549\n",
      "Loss: 1.461225152015686\n",
      "Loss: 1.4622538089752197\n",
      "Loss: 1.4625247716903687\n",
      "Loss: 1.511885643005371\n",
      "Loss: 1.4616140127182007\n",
      "Loss: 1.4680824279785156\n",
      "Loss: 1.462263822555542\n",
      "Loss: 1.5967011451721191\n",
      "Loss: 1.4776638746261597\n",
      "Loss: 1.4611561298370361\n",
      "Loss: 1.5503238439559937\n",
      "Loss: 1.463873028755188\n",
      "Loss: 1.5039305686950684\n",
      "Loss: 1.4617129564285278\n",
      "Loss: 1.4615976810455322\n",
      "Loss: 1.512460470199585\n",
      "Loss: 1.4611997604370117\n",
      "Loss: 1.4612059593200684\n",
      "Loss: 1.462246060371399\n",
      "Loss: 1.4612032175064087\n",
      "Loss: 1.7223973274230957\n",
      "Loss: 1.461255431175232\n",
      "Loss: 1.4611704349517822\n",
      "Loss: 1.4621328115463257\n",
      "Loss: 1.46142578125\n",
      "Loss: 1.462463617324829\n",
      "Loss: 1.4613146781921387\n",
      "Loss: 1.4882290363311768\n",
      "Loss: 1.5609833002090454\n",
      "Loss: 1.461158037185669\n",
      "Loss: 1.475746989250183\n",
      "Loss: 1.4632060527801514\n",
      "Loss: 1.4647183418273926\n",
      "Loss: 1.530962586402893\n",
      "Loss: 1.461639642715454\n",
      "Loss: 1.461251974105835\n",
      "Loss: 1.4622218608856201\n",
      "Loss: 1.4614337682724\n",
      "Loss: 1.5023752450942993\n",
      "Loss: 1.4651402235031128\n",
      "Loss: 1.4668214321136475\n",
      "Loss: 1.4670183658599854\n",
      "Loss: 1.4622776508331299\n",
      "Loss: 1.476365566253662\n",
      "Loss: 1.4616529941558838\n",
      "Loss: 1.4637401103973389\n",
      "Loss: 1.4655430316925049\n",
      "Loss: 1.462235689163208\n",
      "Loss: 1.5083744525909424\n",
      "Loss: 1.4628244638442993\n",
      "Loss: 1.47035813331604\n",
      "Loss: 1.4612126350402832\n",
      "Loss: 1.5332484245300293\n",
      "Loss: 1.4760551452636719\n",
      "Loss: 1.4611730575561523\n",
      "Loss: 1.4613399505615234\n",
      "Loss: 1.461153507232666\n",
      "Loss: 1.4630179405212402\n",
      "Loss: 1.461252212524414\n",
      "Loss: 1.4611986875534058\n",
      "Loss: 1.4612979888916016\n",
      "Loss: 1.5282920598983765\n",
      "Loss: 1.4616059064865112\n",
      "Loss: 1.4611585140228271\n",
      "Loss: 1.4743684530258179\n",
      "Loss: 1.4616144895553589\n",
      "Loss: 1.4646308422088623\n",
      "Loss: 1.4621511697769165\n",
      "Loss: 1.5665748119354248\n",
      "Loss: 1.4755043983459473\n",
      "Loss: 1.481794834136963\n",
      "Loss: 1.4927043914794922\n",
      "Loss: 1.4660060405731201\n",
      "Loss: 1.4619874954223633\n",
      "Loss: 1.4836623668670654\n",
      "Loss: 1.465499758720398\n",
      "Loss: 1.461188554763794\n",
      "Loss: 1.4672542810440063\n",
      "Loss: 1.6176724433898926\n",
      "Loss: 1.4671809673309326\n",
      "Loss: 1.6980640888214111\n",
      "Loss: 1.4655334949493408\n",
      "Loss: 1.5052121877670288\n",
      "Loss: 1.4613440036773682\n",
      "Loss: 1.4786651134490967\n",
      "Loss: 1.4622136354446411\n",
      "Loss: 1.462024211883545\n",
      "Loss: 1.5264970064163208\n",
      "Loss: 1.7342877388000488\n",
      "Loss: 1.4807100296020508\n",
      "Loss: 1.842679738998413\n",
      "Loss: 1.4616607427597046\n",
      "Loss: 1.6839194297790527\n",
      "Loss: 1.8561789989471436\n",
      "Loss: 1.461883544921875\n",
      "Loss: 1.465836524963379\n",
      "Loss: 1.7227803468704224\n",
      "Loss: 1.4839379787445068\n",
      "Loss: 1.7105141878128052\n",
      "Loss: 1.6231322288513184\n",
      "Loss: 1.6448390483856201\n",
      "Loss: 1.577141523361206\n",
      "Loss: 1.4971163272857666\n",
      "Loss: 1.5090473890304565\n",
      "Loss: 1.4612467288970947\n",
      "Loss: 1.4632724523544312\n",
      "Loss: 1.4687023162841797\n",
      "Loss: 1.636212944984436\n",
      "Loss: 1.4800246953964233\n",
      "Loss: 1.4611985683441162\n",
      "Loss: 1.6878398656845093\n",
      "Loss: 1.461249828338623\n",
      "Loss: 1.4676949977874756\n",
      "Loss: 1.493690013885498\n",
      "Loss: 1.4624675512313843\n",
      "Loss: 1.6087044477462769\n",
      "Loss: 1.6355286836624146\n",
      "Loss: 1.494880199432373\n",
      "Loss: 1.4907673597335815\n",
      "Loss: 1.4673354625701904\n",
      "Loss: 1.4633476734161377\n",
      "Loss: 1.817596673965454\n",
      "Loss: 1.4626591205596924\n",
      "Loss: 1.4619094133377075\n",
      "Loss: 1.4890975952148438\n",
      "Loss: 1.7108608484268188\n",
      "Loss: 1.4819055795669556\n",
      "Loss: 1.4953290224075317\n",
      "Loss: 1.461205244064331\n",
      "Loss: 1.4611531496047974\n",
      "Loss: 1.4722628593444824\n",
      "Loss: 1.4617502689361572\n",
      "Loss: 1.461335301399231\n",
      "Loss: 1.4696615934371948\n",
      "Loss: 1.466216802597046\n",
      "Loss: 1.4612007141113281\n",
      "Loss: 1.4612586498260498\n",
      "Loss: 1.4615046977996826\n",
      "Loss: 1.4750045537948608\n",
      "Loss: 1.463637113571167\n",
      "Loss: 1.4612685441970825\n",
      "Loss: 1.4612473249435425\n",
      "Loss: 1.4612284898757935\n",
      "Loss: 1.539921760559082\n",
      "Loss: 1.4726006984710693\n",
      "Loss: 1.4622645378112793\n",
      "Loss: 1.5661500692367554\n",
      "Loss: 1.4972854852676392\n",
      "Loss: 1.5578396320343018\n",
      "Loss: 1.461547613143921\n",
      "Loss: 1.4663691520690918\n",
      "Loss: 1.4676400423049927\n",
      "Loss: 1.5020344257354736\n",
      "Loss: 1.5219528675079346\n",
      "Loss: 1.476992130279541\n",
      "Loss: 1.4729053974151611\n",
      "Loss: 1.5723868608474731\n",
      "Loss: 1.4630696773529053\n",
      "Loss: 1.4612352848052979\n",
      "Loss: 1.7111480236053467\n",
      "Loss: 1.9586013555526733\n",
      "Loss: 1.4613661766052246\n",
      "Loss: 1.4615557193756104\n",
      "Loss: 1.4831538200378418\n",
      "Loss: 1.4782342910766602\n",
      "Loss: 1.4637706279754639\n",
      "Loss: 1.8329980373382568\n",
      "Loss: 1.4617143869400024\n",
      "Loss: 1.6481578350067139\n",
      "Loss: 1.49656343460083\n",
      "Loss: 1.4611650705337524\n",
      "Loss: 1.5863934755325317\n",
      "Loss: 1.7793916463851929\n",
      "Loss: 1.4620225429534912\n",
      "Loss: 1.4660706520080566\n",
      "Loss: 1.4732074737548828\n",
      "Loss: 1.7046149969100952\n",
      "Loss: 1.4663374423980713\n",
      "Loss: 1.4641400575637817\n",
      "Loss: 1.463010311126709\n",
      "Loss: 1.4616400003433228\n",
      "Loss: 1.9195780754089355\n",
      "Loss: 1.4678494930267334\n",
      "Loss: 1.4954676628112793\n",
      "Loss: 1.5165923833847046\n",
      "Loss: 1.462870478630066\n",
      "Loss: 1.503474235534668\n",
      "Loss: 1.4728028774261475\n",
      "Loss: 1.4738906621932983\n",
      "Loss: 1.5303398370742798\n",
      "Loss: 1.470312476158142\n",
      "Loss: 1.4613401889801025\n",
      "Loss: 1.5041892528533936\n",
      "Loss: 1.708627700805664\n",
      "Loss: 1.5605149269104004\n",
      "Loss: 1.4659570455551147\n",
      "Loss: 1.8073935508728027\n",
      "Loss: 1.6125396490097046\n",
      "Loss: 1.7793872356414795\n",
      "Loss: 1.6585333347320557\n",
      "Loss: 1.4641684293746948\n",
      "Loss: 1.461516261100769\n",
      "Loss: 1.838403344154358\n",
      "Loss: 1.4615532159805298\n",
      "Loss: 1.5527238845825195\n",
      "Loss: 1.7053532600402832\n",
      "Loss: 1.4673289060592651\n",
      "Loss: 1.461305022239685\n",
      "Loss: 1.7088687419891357\n",
      "Loss: 1.5676484107971191\n",
      "Loss: 1.7036457061767578\n",
      "Loss: 1.5279568433761597\n",
      "Loss: 1.461195468902588\n",
      "Loss: 1.465014934539795\n",
      "Loss: 1.4611629247665405\n",
      "Loss: 1.7208157777786255\n",
      "Loss: 1.6296958923339844\n",
      "Loss: 1.4612188339233398\n",
      "Loss: 1.4615025520324707\n",
      "Loss: 1.7355432510375977\n",
      "Loss: 1.4611501693725586\n",
      "Loss: 1.4614028930664062\n",
      "Loss: 1.4611961841583252\n",
      "Loss: 1.461195468902588\n",
      "Loss: 1.4615044593811035\n",
      "Loss: 1.4611682891845703\n",
      "Loss: 1.4625234603881836\n",
      "Loss: 1.7058584690093994\n",
      "Loss: 1.4882442951202393\n",
      "Loss: 1.7020156383514404\n",
      "Loss: 1.7074326276779175\n",
      "Loss: 2.061882734298706\n",
      "Loss: 1.461289405822754\n",
      "Loss: 1.4611879587173462\n",
      "Loss: 1.4611763954162598\n",
      "Loss: 1.4629840850830078\n",
      "Loss: 1.46383798122406\n",
      "Loss: 1.4616953134536743\n",
      "Loss: 1.4612071514129639\n",
      "Loss: 1.4611706733703613\n",
      "Loss: 1.462107539176941\n",
      "Loss: 1.46173894405365\n",
      "Loss: 1.4619392156600952\n",
      "Loss: 1.4694528579711914\n",
      "Loss: 1.4626410007476807\n",
      "Loss: 1.4612528085708618\n",
      "Loss: 1.4614198207855225\n",
      "Loss: 1.4796686172485352\n",
      "Loss: 1.4656652212142944\n",
      "Loss: 1.4612516164779663\n",
      "Loss: 1.4616352319717407\n",
      "Loss: 1.4615992307662964\n",
      "Loss: 1.4611896276474\n",
      "Loss: 1.461243987083435\n",
      "Loss: 1.4643380641937256\n",
      "Loss: 1.4711978435516357\n",
      "Loss: 1.4612524509429932\n",
      "Loss: 1.461245059967041\n",
      "Loss: 1.4611724615097046\n",
      "Loss: 1.461158037185669\n",
      "Loss: 1.4621440172195435\n",
      "Loss: 1.463902235031128\n",
      "Loss: 1.462003469467163\n",
      "Loss: 1.4620660543441772\n",
      "Loss: 1.5648362636566162\n",
      "Loss: 1.461670160293579\n",
      "Loss: 1.461332082748413\n",
      "Loss: 1.4611705541610718\n",
      "Loss: 1.4614536762237549\n",
      "Loss: 1.4664462804794312\n",
      "Loss: 1.4613995552062988\n",
      "Loss: 1.4611551761627197\n",
      "Loss: 1.4613555669784546\n",
      "Loss: 1.4614711999893188\n",
      "Loss: 1.6889283657073975\n",
      "Loss: 1.4614264965057373\n",
      "Loss: 1.461152195930481\n",
      "Loss: 1.4667161703109741\n",
      "Loss: 1.4611846208572388\n",
      "Loss: 1.461159348487854\n",
      "Loss: 1.462709665298462\n",
      "Loss: 1.462494134902954\n",
      "Loss: 1.4647932052612305\n",
      "Loss: 1.4611707925796509\n",
      "Loss: 1.4621031284332275\n",
      "Loss: 1.7110964059829712\n",
      "Loss: 1.4715945720672607\n",
      "Loss: 1.461841344833374\n",
      "Loss: 1.4979723691940308\n",
      "Loss: 1.4611940383911133\n",
      "Loss: 1.4637775421142578\n",
      "Loss: 1.461150884628296\n",
      "Loss: 1.570804238319397\n",
      "Loss: 1.672476053237915\n",
      "Loss: 1.8177543878555298\n",
      "Loss: 1.4805227518081665\n",
      "Loss: 1.566828727722168\n",
      "Loss: 1.4613661766052246\n",
      "Loss: 1.4670644998550415\n",
      "Loss: 1.466968297958374\n",
      "Loss: 1.465386986732483\n",
      "Loss: 1.627281904220581\n",
      "Loss: 1.4823760986328125\n",
      "Loss: 1.4611514806747437\n",
      "Loss: 1.461749792098999\n",
      "Loss: 1.4625760316848755\n",
      "Loss: 1.4616856575012207\n",
      "Loss: 1.4611657857894897\n",
      "Loss: 1.7051856517791748\n",
      "Loss: 1.4614288806915283\n",
      "Loss: 1.461945652961731\n",
      "Loss: 1.5880860090255737\n",
      "Loss: 1.4612401723861694\n",
      "Loss: 1.6233776807785034\n",
      "Loss: 1.7133938074111938\n",
      "Loss: 1.599421501159668\n",
      "Loss: 1.461930513381958\n",
      "Loss: 1.6910942792892456\n",
      "Loss: 1.497839093208313\n",
      "Loss: 1.4630485773086548\n",
      "Loss: 1.4905236959457397\n",
      "Loss: 1.4756855964660645\n",
      "Loss: 1.4641594886779785\n",
      "Loss: 1.4690358638763428\n",
      "Loss: 1.4629087448120117\n",
      "Loss: 1.4613994359970093\n",
      "Loss: 1.7132304906845093\n",
      "Loss: 1.604517936706543\n",
      "Loss: 1.6825284957885742\n",
      "Loss: 1.5270274877548218\n",
      "Loss: 1.947136402130127\n",
      "Loss: 1.9500271081924438\n",
      "Loss: 1.8619965314865112\n",
      "Loss: 1.4630345106124878\n",
      "Loss: 1.4797043800354004\n",
      "Loss: 1.4616360664367676\n",
      "Loss: 1.4658418893814087\n",
      "Loss: 1.965649962425232\n",
      "Loss: 1.6941440105438232\n",
      "Loss: 1.4743132591247559\n",
      "Loss: 1.552087664604187\n",
      "Loss: 1.49472177028656\n",
      "Loss: 1.471683144569397\n",
      "Loss: 1.464271068572998\n",
      "Loss: 1.7144544124603271\n",
      "Loss: 1.4619896411895752\n",
      "Loss: 1.6440531015396118\n",
      "Loss: 1.4620782136917114\n",
      "Loss: 1.8325939178466797\n",
      "Loss: 1.6067168712615967\n",
      "Loss: 1.703763723373413\n",
      "Loss: 1.461913824081421\n",
      "Loss: 1.6451581716537476\n",
      "Loss: 1.4680942296981812\n",
      "Loss: 1.4631471633911133\n",
      "Loss: 1.5068424940109253\n",
      "Loss: 1.4646275043487549\n",
      "Loss: 1.461471438407898\n",
      "Loss: 1.461761713027954\n",
      "Loss: 1.4811785221099854\n",
      "Loss: 1.4722288846969604\n",
      "Loss: 1.4689254760742188\n",
      "Loss: 1.4630624055862427\n",
      "Loss: 1.46123468875885\n",
      "Loss: 1.4825036525726318\n",
      "Loss: 1.4613311290740967\n",
      "Loss: 1.4614030122756958\n",
      "Loss: 1.6330170631408691\n",
      "Loss: 1.7028801441192627\n",
      "Loss: 1.4624381065368652\n",
      "Loss: 1.463510274887085\n",
      "Loss: 1.4614429473876953\n",
      "Loss: 1.4912338256835938\n",
      "Loss: 1.7218877077102661\n",
      "Loss: 1.9621217250823975\n",
      "Loss: 1.4672555923461914\n",
      "Loss: 1.5526760816574097\n",
      "Loss: 1.4661632776260376\n",
      "Loss: 1.471435308456421\n",
      "Loss: 1.4619619846343994\n",
      "Loss: 1.8875616788864136\n",
      "Loss: 1.7056565284729004\n",
      "Loss: 1.4720412492752075\n",
      "Loss: 1.49072265625\n",
      "Loss: 1.73695707321167\n",
      "Loss: 1.501227617263794\n",
      "Loss: 1.4622634649276733\n",
      "Loss: 1.7107781171798706\n",
      "Loss: 1.4623663425445557\n",
      "Loss: 1.4757072925567627\n",
      "Loss: 1.4622822999954224\n",
      "Loss: 1.4621409177780151\n",
      "Loss: 1.597930908203125\n",
      "Loss: 1.4611835479736328\n",
      "Loss: 1.4890143871307373\n",
      "Loss: 1.461388349533081\n",
      "Loss: 1.4639009237289429\n",
      "Loss: 1.4661414623260498\n",
      "Loss: 1.4612053632736206\n",
      "Loss: 1.5989127159118652\n",
      "Loss: 1.4621775150299072\n",
      "Loss: 1.4772225618362427\n",
      "Loss: 1.4614406824111938\n",
      "Loss: 1.461172103881836\n",
      "Loss: 1.4792125225067139\n",
      "Loss: 1.4645415544509888\n",
      "Loss: 1.495154619216919\n",
      "Loss: 1.467491626739502\n",
      "Loss: 1.4629740715026855\n",
      "Loss: 1.5882155895233154\n",
      "Loss: 1.461883544921875\n",
      "Loss: 1.4857711791992188\n",
      "Loss: 1.4797120094299316\n",
      "Loss: 1.4614946842193604\n",
      "Loss: 1.6047676801681519\n",
      "Loss: 1.5233097076416016\n",
      "Loss: 1.4904390573501587\n",
      "Loss: 1.54176926612854\n",
      "Loss: 1.4614882469177246\n",
      "Loss: 1.6122443675994873\n",
      "Loss: 1.462377905845642\n",
      "Loss: 1.4612269401550293\n",
      "Loss: 1.4613977670669556\n",
      "Loss: 1.467375636100769\n",
      "Loss: 1.5506336688995361\n",
      "Loss: 1.4611866474151611\n",
      "Loss: 1.464874505996704\n",
      "Loss: 1.461532711982727\n",
      "Loss: 1.4613511562347412\n",
      "Loss: 1.4616727828979492\n",
      "Loss: 1.461983561515808\n",
      "Loss: 1.461281180381775\n",
      "Loss: 1.461223840713501\n",
      "Loss: 1.5097510814666748\n",
      "Loss: 1.4612364768981934\n",
      "Loss: 1.4750630855560303\n",
      "Loss: 1.4611546993255615\n",
      "Loss: 1.4673523902893066\n",
      "Loss: 1.491788387298584\n",
      "Loss: 1.4617180824279785\n",
      "Loss: 1.4649817943572998\n",
      "Loss: 1.4611613750457764\n",
      "Loss: 1.4641306400299072\n",
      "Loss: 1.4617705345153809\n",
      "Loss: 1.4616743326187134\n",
      "Loss: 1.4850282669067383\n",
      "Loss: 1.6320213079452515\n",
      "Loss: 1.4612194299697876\n",
      "Loss: 1.477028727531433\n",
      "Loss: 1.4708514213562012\n",
      "Loss: 1.4745632410049438\n",
      "Loss: 1.4625277519226074\n",
      "Loss: 1.4613027572631836\n",
      "Loss: 1.4761056900024414\n",
      "Loss: 1.461203694343567\n",
      "Loss: 1.4658598899841309\n",
      "Loss: 1.4616414308547974\n",
      "Loss: 1.461161732673645\n",
      "Loss: 1.5014286041259766\n",
      "Loss: 1.4620115756988525\n",
      "Loss: 1.466760277748108\n",
      "Loss: 1.6213326454162598\n",
      "Loss: 1.461535930633545\n",
      "Loss: 1.4649893045425415\n",
      "Loss: 1.507526159286499\n",
      "Loss: 1.4611692428588867\n",
      "Loss: 1.4611612558364868\n",
      "Loss: 1.4638803005218506\n",
      "Loss: 1.6880173683166504\n",
      "Loss: 1.4622406959533691\n",
      "Loss: 1.7081650495529175\n",
      "Loss: 1.4621331691741943\n",
      "Loss: 1.4628815650939941\n",
      "Loss: 1.4638469219207764\n",
      "Loss: 1.461380958557129\n",
      "Loss: 1.4612776041030884\n",
      "Loss: 1.5629754066467285\n",
      "Loss: 1.4612298011779785\n",
      "Loss: 1.461187481880188\n",
      "Loss: 1.4614272117614746\n",
      "Loss: 1.4613454341888428\n",
      "Loss: 1.4642093181610107\n",
      "Loss: 1.4676899909973145\n",
      "Loss: 1.4713612794876099\n",
      "Loss: 1.4629600048065186\n",
      "Loss: 1.464318037033081\n",
      "Loss: 1.6140203475952148\n",
      "Loss: 1.557555913925171\n",
      "Loss: 1.4681626558303833\n",
      "Loss: 1.4623785018920898\n",
      "Loss: 1.520246148109436\n",
      "Loss: 1.6766430139541626\n",
      "Loss: 1.657304048538208\n",
      "Loss: 1.6863585710525513\n",
      "Loss: 1.4638502597808838\n",
      "Loss: 1.4637870788574219\n",
      "Loss: 1.486262321472168\n",
      "Loss: 1.4638618230819702\n",
      "Loss: 1.7121102809906006\n",
      "Loss: 1.462632179260254\n",
      "Loss: 1.579866886138916\n",
      "Loss: 1.46144700050354\n",
      "Loss: 1.4760520458221436\n",
      "Loss: 1.4825503826141357\n",
      "Loss: 1.6143718957901\n",
      "Loss: 1.4817883968353271\n",
      "Loss: 1.4626593589782715\n",
      "Loss: 1.463511347770691\n",
      "Loss: 1.4614088535308838\n",
      "Loss: 1.4623419046401978\n",
      "Loss: 1.4614142179489136\n",
      "Loss: 1.4637537002563477\n",
      "Loss: 1.4614744186401367\n",
      "Loss: 1.4621304273605347\n",
      "Loss: 1.4793258905410767\n",
      "Loss: 1.461626648902893\n",
      "Loss: 1.4612600803375244\n",
      "Loss: 1.4612106084823608\n",
      "Loss: 1.4807201623916626\n",
      "Loss: 1.4612829685211182\n",
      "Loss: 1.4611506462097168\n",
      "Loss: 1.4670535326004028\n",
      "Loss: 1.7091187238693237\n",
      "Loss: 1.4615392684936523\n",
      "Loss: 1.4613758325576782\n",
      "Loss: 1.4628269672393799\n",
      "Loss: 1.4615116119384766\n",
      "Loss: 1.46894109249115\n",
      "Loss: 1.4619288444519043\n",
      "Loss: 1.4625353813171387\n",
      "Loss: 1.462883472442627\n",
      "Loss: 1.4617564678192139\n",
      "Loss: 1.4617239236831665\n",
      "Loss: 1.4616469144821167\n",
      "Loss: 1.4629312753677368\n",
      "Loss: 1.4775141477584839\n",
      "Loss: 1.5057083368301392\n",
      "Loss: 1.4613701105117798\n",
      "Loss: 1.4676960706710815\n",
      "Loss: 1.4623208045959473\n",
      "Loss: 1.462314486503601\n",
      "Loss: 1.4635436534881592\n",
      "Loss: 1.486271858215332\n",
      "Loss: 1.464706540107727\n",
      "Loss: 1.501086711883545\n",
      "Loss: 1.4678891897201538\n",
      "Loss: 1.9748353958129883\n",
      "Loss: 1.5136486291885376\n",
      "Loss: 1.462643027305603\n",
      "Loss: 1.5076897144317627\n",
      "Loss: 1.702418327331543\n",
      "Loss: 1.6809405088424683\n",
      "Loss: 1.6319670677185059\n",
      "Loss: 1.4618533849716187\n",
      "Loss: 1.4655680656433105\n",
      "Loss: 1.4612812995910645\n",
      "Loss: 1.5921790599822998\n",
      "Loss: 1.4887489080429077\n",
      "Loss: 1.504150152206421\n",
      "Loss: 1.4717024564743042\n",
      "Loss: 1.4625840187072754\n",
      "Loss: 1.7673145532608032\n",
      "Loss: 1.6983532905578613\n",
      "Loss: 1.4707283973693848\n",
      "Loss: 1.4632197618484497\n",
      "Loss: 1.510977029800415\n",
      "Loss: 1.5150829553604126\n",
      "Loss: 1.4613025188446045\n",
      "Loss: 1.5101348161697388\n",
      "Loss: 1.4717161655426025\n",
      "Loss: 1.4655590057373047\n",
      "Loss: 1.4619359970092773\n",
      "Loss: 1.706664800643921\n",
      "Loss: 1.9316797256469727\n",
      "Loss: 1.651426911354065\n",
      "Loss: 1.461424708366394\n",
      "Loss: 1.5117162466049194\n",
      "Loss: 1.4614251852035522\n",
      "Loss: 1.4618549346923828\n",
      "Loss: 1.6332170963287354\n",
      "Loss: 1.4627394676208496\n",
      "Loss: 1.4614468812942505\n",
      "Loss: 1.4866647720336914\n",
      "Loss: 1.488122582435608\n",
      "Loss: 1.4656236171722412\n",
      "Loss: 1.4621269702911377\n",
      "Loss: 1.4685853719711304\n",
      "Loss: 1.4884008169174194\n",
      "Loss: 1.6629695892333984\n",
      "Loss: 1.4612706899642944\n",
      "Loss: 1.462226152420044\n",
      "Loss: 1.4615068435668945\n",
      "Loss: 1.6100558042526245\n",
      "Loss: 1.4702171087265015\n",
      "Loss: 1.4612098932266235\n",
      "Loss: 1.4616267681121826\n",
      "Loss: 1.4839622974395752\n",
      "Loss: 1.7167086601257324\n",
      "Loss: 1.5887296199798584\n",
      "Loss: 1.4656866788864136\n",
      "Loss: 1.5509649515151978\n",
      "Loss: 1.461879014968872\n",
      "Loss: 1.4927921295166016\n",
      "Loss: 1.4627785682678223\n",
      "Loss: 1.4612762928009033\n",
      "Loss: 1.4630179405212402\n",
      "Loss: 1.4758251905441284\n",
      "Loss: 1.6409790515899658\n",
      "Loss: 1.4643384218215942\n",
      "Loss: 1.4613935947418213\n",
      "Loss: 1.623111605644226\n",
      "Loss: 1.5835182666778564\n",
      "Loss: 1.472985863685608\n",
      "Loss: 1.461708426475525\n",
      "Loss: 1.4634284973144531\n",
      "Loss: 1.461535096168518\n",
      "Loss: 1.5715563297271729\n",
      "Loss: 1.4646967649459839\n",
      "Loss: 1.6367686986923218\n",
      "Loss: 1.6609138250350952\n",
      "Loss: 1.4705826044082642\n",
      "Loss: 1.4613364934921265\n",
      "Loss: 1.4949498176574707\n",
      "Loss: 1.4616508483886719\n",
      "Loss: 1.4612865447998047\n",
      "Loss: 1.461287021636963\n",
      "Loss: 1.466334581375122\n",
      "Loss: 1.4640185832977295\n",
      "Loss: 1.4611612558364868\n",
      "Loss: 1.4661823511123657\n",
      "Loss: 1.4613730907440186\n",
      "Loss: 1.4633835554122925\n",
      "Loss: 1.464299201965332\n",
      "Loss: 1.4618432521820068\n",
      "Loss: 1.4662251472473145\n",
      "Loss: 1.4619076251983643\n",
      "Loss: 1.4613503217697144\n",
      "Loss: 1.7055069208145142\n",
      "Loss: 1.709993600845337\n",
      "Loss: 1.4624032974243164\n",
      "Loss: 1.626175880432129\n",
      "Loss: 1.5671055316925049\n",
      "Loss: 1.462691307067871\n",
      "Loss: 1.8277958631515503\n",
      "Loss: 1.710515022277832\n",
      "Loss: 1.461359977722168\n",
      "Loss: 1.4625903367996216\n",
      "Loss: 1.5223455429077148\n",
      "Loss: 1.7078158855438232\n",
      "Loss: 1.5724999904632568\n",
      "Loss: 2.170727491378784\n",
      "Loss: 1.4617888927459717\n",
      "Loss: 2.29512619972229\n",
      "Loss: 1.5884907245635986\n",
      "Loss: 1.4613326787948608\n",
      "Loss: 1.7114887237548828\n",
      "Loss: 1.4683375358581543\n",
      "Loss: 1.6833789348602295\n",
      "Loss: 1.4611661434173584\n",
      "Loss: 1.7021831274032593\n",
      "Loss: 1.7116618156433105\n",
      "Loss: 1.4611706733703613\n",
      "Loss: 1.7100026607513428\n",
      "Loss: 1.5335553884506226\n",
      "Loss: 1.7110636234283447\n",
      "Loss: 1.4668256044387817\n",
      "Loss: 1.4623199701309204\n",
      "Loss: 1.9686050415039062\n",
      "Loss: 1.6242239475250244\n",
      "Loss: 1.461978793144226\n",
      "Loss: 1.707529067993164\n",
      "Loss: 1.463257074356079\n",
      "Loss: 1.4611515998840332\n",
      "Loss: 1.461194634437561\n",
      "Loss: 1.7097370624542236\n",
      "Loss: 1.5774297714233398\n",
      "Loss: 1.4639750719070435\n",
      "Loss: 1.4611797332763672\n",
      "Loss: 1.461176872253418\n",
      "Loss: 1.4639533758163452\n",
      "Loss: 1.4790862798690796\n",
      "Loss: 1.4614715576171875\n",
      "Loss: 1.4611504077911377\n",
      "Loss: 1.4613107442855835\n",
      "Loss: 1.4612094163894653\n",
      "Loss: 1.5770906209945679\n",
      "Loss: 1.4611512422561646\n",
      "Loss: 1.6066083908081055\n",
      "Loss: 1.5646982192993164\n",
      "Loss: 1.4812616109848022\n",
      "Loss: 1.5702637434005737\n",
      "Loss: 1.4611647129058838\n",
      "Loss: 1.4611544609069824\n",
      "Loss: 1.698258876800537\n",
      "Loss: 1.4611543416976929\n",
      "Loss: 1.4614397287368774\n",
      "Loss: 1.4612020254135132\n",
      "Loss: 1.461175560951233\n",
      "Loss: 1.4612160921096802\n",
      "Loss: 1.6863751411437988\n",
      "Loss: 1.4626133441925049\n",
      "Loss: 1.461150884628296\n",
      "Loss: 1.5488094091415405\n",
      "Loss: 1.6211910247802734\n",
      "Loss: 1.4665501117706299\n",
      "Loss: 1.48284912109375\n",
      "Loss: 1.5109870433807373\n",
      "Loss: 1.4619941711425781\n",
      "Loss: 1.6924700736999512\n",
      "Loss: 1.4616923332214355\n",
      "Loss: 1.7106027603149414\n",
      "Loss: 1.949088454246521\n",
      "Loss: 1.4958354234695435\n",
      "Loss: 1.4616611003875732\n",
      "Loss: 1.4655026197433472\n",
      "Loss: 1.4614059925079346\n",
      "Loss: 1.4807937145233154\n",
      "Loss: 1.462437391281128\n",
      "Loss: 1.4614120721817017\n",
      "Loss: 1.461426854133606\n",
      "Loss: 1.4613398313522339\n",
      "Loss: 1.4620082378387451\n",
      "Loss: 1.4629449844360352\n",
      "Loss: 1.4693704843521118\n",
      "Loss: 1.4666612148284912\n",
      "Loss: 1.462475061416626\n",
      "Loss: 1.4655404090881348\n",
      "Loss: 1.4665355682373047\n",
      "Loss: 1.4613511562347412\n",
      "Loss: 1.6899112462997437\n",
      "Loss: 1.4672728776931763\n",
      "Loss: 1.461307168006897\n",
      "Loss: 1.4633883237838745\n",
      "Loss: 1.780566930770874\n",
      "Loss: 1.4645863771438599\n",
      "Loss: 1.4612611532211304\n",
      "Loss: 1.4612027406692505\n",
      "Loss: 1.771226167678833\n",
      "Loss: 1.481054663658142\n",
      "Loss: 1.4629638195037842\n",
      "Loss: 1.4630366563796997\n",
      "Loss: 1.4611749649047852\n",
      "Loss: 1.4708847999572754\n",
      "Loss: 1.4614330530166626\n",
      "Loss: 1.4614777565002441\n",
      "Loss: 1.4773802757263184\n",
      "Loss: 1.4632834196090698\n",
      "Loss: 1.4623737335205078\n",
      "Loss: 1.4615812301635742\n",
      "Loss: 1.690873146057129\n",
      "Loss: 1.4611659049987793\n",
      "Loss: 1.5029358863830566\n",
      "Loss: 1.4670010805130005\n",
      "Loss: 1.4917337894439697\n",
      "Loss: 1.4731210470199585\n",
      "Loss: 1.4616572856903076\n",
      "Loss: 1.6603962182998657\n",
      "Loss: 1.891982913017273\n",
      "Loss: 1.4614371061325073\n",
      "Loss: 1.5362842082977295\n",
      "Loss: 1.5138970613479614\n",
      "Loss: 1.916546106338501\n",
      "Loss: 1.6034314632415771\n",
      "Loss: 1.4793721437454224\n",
      "Loss: 1.4618198871612549\n",
      "Loss: 1.7072314023971558\n",
      "Loss: 1.4824318885803223\n",
      "Loss: 1.4910167455673218\n",
      "Loss: 1.4681177139282227\n",
      "Loss: 1.4732415676116943\n",
      "Loss: 1.4611607789993286\n",
      "Loss: 1.599992036819458\n",
      "Loss: 1.6986732482910156\n",
      "Loss: 1.461395025253296\n",
      "Loss: 1.461432695388794\n",
      "Loss: 1.461310863494873\n",
      "Loss: 1.7008105516433716\n",
      "Loss: 1.4632213115692139\n",
      "Loss: 1.6536495685577393\n",
      "Loss: 1.4617053270339966\n",
      "Loss: 1.461189866065979\n",
      "Loss: 1.4740046262741089\n",
      "Loss: 1.4668240547180176\n",
      "Loss: 1.4617078304290771\n",
      "Loss: 1.4638676643371582\n",
      "Loss: 1.4661431312561035\n",
      "Loss: 1.4612650871276855\n",
      "Loss: 1.4617507457733154\n",
      "Loss: 1.4612071514129639\n",
      "Loss: 1.662024736404419\n",
      "Loss: 1.8751823902130127\n",
      "Loss: 1.4947750568389893\n",
      "Loss: 1.4715430736541748\n",
      "Loss: 1.465097188949585\n",
      "Loss: 1.6292190551757812\n",
      "Loss: 1.4642196893692017\n",
      "Loss: 1.4618399143218994\n",
      "Loss: 1.4616889953613281\n",
      "Loss: 1.4635553359985352\n",
      "Loss: 1.6837470531463623\n",
      "Loss: 1.4611611366271973\n",
      "Loss: 1.5850934982299805\n",
      "Loss: 1.740006446838379\n",
      "Loss: 1.461195468902588\n",
      "Loss: 1.46165931224823\n",
      "Loss: 1.4615302085876465\n",
      "Loss: 1.4611520767211914\n",
      "Loss: 1.4860466718673706\n",
      "Loss: 1.4611514806747437\n",
      "Loss: 1.5176446437835693\n",
      "Loss: 1.4617705345153809\n",
      "Loss: 1.4626922607421875\n",
      "Loss: 1.4612222909927368\n",
      "Loss: 1.4613431692123413\n",
      "Loss: 1.5401611328125\n",
      "Loss: 1.4657423496246338\n",
      "Loss: 1.4611830711364746\n",
      "Loss: 1.4623351097106934\n",
      "Loss: 2.1149961948394775\n",
      "Loss: 1.4614973068237305\n",
      "Loss: 1.4743338823318481\n",
      "Loss: 1.4613029956817627\n",
      "Loss: 1.4613161087036133\n",
      "Loss: 1.4611896276474\n",
      "Loss: 1.4616968631744385\n",
      "Loss: 1.4611690044403076\n",
      "Loss: 1.4656760692596436\n",
      "Loss: 1.4615470170974731\n",
      "Loss: 1.4612606763839722\n",
      "Loss: 1.4617024660110474\n",
      "Loss: 1.4613155126571655\n",
      "Loss: 1.4643000364303589\n",
      "Loss: 1.4640545845031738\n",
      "Loss: 1.4618301391601562\n",
      "Loss: 1.4642447233200073\n",
      "Loss: 1.4618499279022217\n",
      "Loss: 1.4619802236557007\n",
      "Loss: 1.4619418382644653\n",
      "Loss: 1.4612934589385986\n",
      "Loss: 1.4760863780975342\n",
      "Loss: 1.4628456830978394\n",
      "Loss: 1.4812183380126953\n",
      "Loss: 1.483967661857605\n",
      "Loss: 1.462845802307129\n",
      "Loss: 1.4618829488754272\n",
      "Loss: 1.4615312814712524\n",
      "Loss: 1.462662696838379\n",
      "Loss: 1.7119933366775513\n",
      "Loss: 1.4619629383087158\n",
      "Loss: 1.4614779949188232\n",
      "Loss: 1.4657882452011108\n",
      "Loss: 1.462585210800171\n",
      "Loss: 1.4614112377166748\n",
      "Loss: 1.4625003337860107\n",
      "Loss: 1.46229887008667\n",
      "Loss: 1.47842276096344\n",
      "Loss: 1.4612772464752197\n",
      "Loss: 1.4628517627716064\n",
      "Loss: 1.4611769914627075\n",
      "Loss: 1.4613103866577148\n",
      "Loss: 1.4612935781478882\n",
      "Loss: 1.4612680673599243\n",
      "Loss: 1.461530327796936\n",
      "Loss: 1.4613499641418457\n",
      "Loss: 1.4612557888031006\n",
      "Loss: 1.4612621068954468\n",
      "Loss: 1.4613189697265625\n",
      "Loss: 1.4611878395080566\n",
      "Loss: 1.4650647640228271\n",
      "Loss: 1.4623327255249023\n",
      "Loss: 1.4671558141708374\n",
      "Loss: 1.4763462543487549\n",
      "Loss: 1.4611613750457764\n",
      "Loss: 1.461324691772461\n",
      "Loss: 1.4612271785736084\n",
      "Loss: 1.4613685607910156\n",
      "Loss: 1.4611648321151733\n",
      "Loss: 1.4748213291168213\n",
      "Loss: 1.461208701133728\n",
      "Loss: 1.4611760377883911\n",
      "Loss: 1.4613735675811768\n",
      "Loss: 1.4611725807189941\n",
      "Loss: 1.4615771770477295\n",
      "Loss: 1.4620976448059082\n",
      "Loss: 1.463110089302063\n",
      "Loss: 1.4611785411834717\n",
      "Loss: 1.4612770080566406\n",
      "Loss: 1.4612401723861694\n",
      "Loss: 1.461169958114624\n",
      "Loss: 1.461552619934082\n",
      "Loss: 1.462499737739563\n",
      "Loss: 1.461181402206421\n",
      "Loss: 1.4862267971038818\n",
      "Loss: 1.4613591432571411\n",
      "Loss: 1.4617769718170166\n",
      "Loss: 1.4625470638275146\n",
      "Loss: 1.4659075736999512\n",
      "Loss: 1.463857889175415\n",
      "Loss: 1.4615068435668945\n",
      "Loss: 1.4638242721557617\n",
      "Loss: 1.4612011909484863\n",
      "Loss: 1.4611701965332031\n",
      "Loss: 1.4617326259613037\n",
      "Loss: 1.4757914543151855\n",
      "Loss: 1.4615600109100342\n",
      "Loss: 1.46115243434906\n",
      "Loss: 1.4613007307052612\n",
      "Loss: 1.4612267017364502\n",
      "Loss: 1.4723767042160034\n",
      "Loss: 1.4623037576675415\n",
      "Loss: 1.4614697694778442\n",
      "Loss: 1.4621531963348389\n",
      "Loss: 1.4613897800445557\n",
      "Loss: 1.4611632823944092\n",
      "Loss: 1.4618842601776123\n",
      "Loss: 1.4612818956375122\n",
      "Loss: 1.4642889499664307\n",
      "Loss: 1.4614343643188477\n",
      "Loss: 1.4613244533538818\n",
      "Loss: 1.4614216089248657\n",
      "Loss: 1.4687508344650269\n",
      "Loss: 1.461287021636963\n",
      "Loss: 1.4612890481948853\n",
      "Loss: 1.4899641275405884\n",
      "Loss: 1.4612159729003906\n",
      "Loss: 1.4615389108657837\n",
      "Loss: 1.5684309005737305\n",
      "Loss: 1.4667000770568848\n",
      "Loss: 1.4624208211898804\n",
      "Loss: 1.4611972570419312\n",
      "Loss: 1.4611928462982178\n",
      "Loss: 1.4614001512527466\n",
      "Loss: 1.4612075090408325\n",
      "Loss: 1.4615535736083984\n",
      "Loss: 1.466079592704773\n",
      "Loss: 1.4611948728561401\n",
      "Loss: 1.461326003074646\n",
      "Loss: 1.4614200592041016\n",
      "Loss: 1.4613831043243408\n",
      "Loss: 1.6274296045303345\n",
      "Loss: 1.9637348651885986\n",
      "Loss: 1.9602611064910889\n",
      "Loss: 1.7268718481063843\n",
      "Loss: 1.4989886283874512\n",
      "Loss: 1.7459790706634521\n",
      "Loss: 1.6503188610076904\n",
      "Loss: 1.4795678853988647\n",
      "Loss: 1.7112234830856323\n",
      "Loss: 1.4928779602050781\n",
      "Loss: 1.856980800628662\n",
      "Loss: 1.4646432399749756\n",
      "Loss: 1.463008165359497\n",
      "Loss: 1.4666835069656372\n",
      "Loss: 1.4614272117614746\n",
      "Loss: 1.4611530303955078\n",
      "Loss: 1.4675979614257812\n",
      "Loss: 1.4611828327178955\n",
      "Loss: 1.4614888429641724\n",
      "Loss: 1.4671508073806763\n",
      "Loss: 1.4614555835723877\n",
      "Loss: 1.46510648727417\n",
      "Loss: 1.4693939685821533\n",
      "Loss: 1.4611788988113403\n",
      "Loss: 1.4808645248413086\n",
      "Loss: 1.4643679857254028\n",
      "Loss: 1.6979336738586426\n",
      "Loss: 1.4612993001937866\n",
      "Loss: 1.481438398361206\n",
      "Loss: 1.4613637924194336\n",
      "Loss: 1.4619077444076538\n",
      "Loss: 1.4647676944732666\n",
      "Loss: 1.462230920791626\n",
      "Loss: 1.5506592988967896\n",
      "Loss: 1.6806765794754028\n",
      "Loss: 1.4612329006195068\n",
      "Loss: 1.4708614349365234\n",
      "Loss: 1.4661957025527954\n",
      "Loss: 1.4611682891845703\n",
      "Loss: 1.5088516473770142\n",
      "Loss: 1.4625427722930908\n",
      "Loss: 1.6096948385238647\n",
      "Loss: 1.4679436683654785\n",
      "Loss: 1.461154580116272\n",
      "Loss: 1.7004690170288086\n",
      "Loss: 1.4614055156707764\n",
      "Loss: 1.4620873928070068\n",
      "Loss: 1.4620203971862793\n",
      "Loss: 1.4631344079971313\n",
      "Loss: 1.468023657798767\n",
      "Loss: 1.4638516902923584\n",
      "Loss: 1.8387632369995117\n",
      "Loss: 1.4654275178909302\n",
      "Loss: 1.46532142162323\n",
      "Loss: 1.4612009525299072\n",
      "Loss: 1.4683895111083984\n",
      "Loss: 1.4780977964401245\n",
      "Loss: 1.4643489122390747\n",
      "Loss: 1.4651782512664795\n",
      "Loss: 1.4615033864974976\n",
      "Loss: 1.4629969596862793\n",
      "Loss: 1.4669487476348877\n",
      "Loss: 1.462913990020752\n",
      "Loss: 1.4634126424789429\n",
      "Loss: 1.4664056301116943\n",
      "Loss: 1.4623844623565674\n",
      "Loss: 1.4678269624710083\n",
      "Loss: 1.461609959602356\n",
      "Loss: 1.461340069770813\n",
      "Loss: 1.7103278636932373\n",
      "Loss: 1.4651542901992798\n",
      "Loss: 1.463369607925415\n",
      "Loss: 1.4625611305236816\n",
      "Loss: 1.4619431495666504\n",
      "Loss: 1.4916822910308838\n",
      "Loss: 1.4621201753616333\n",
      "Loss: 1.4829740524291992\n",
      "Loss: 1.4662284851074219\n",
      "Loss: 1.6809877157211304\n",
      "Loss: 1.4779080152511597\n",
      "Loss: 1.4655917882919312\n",
      "Loss: 1.4630870819091797\n",
      "Loss: 1.4616222381591797\n",
      "Loss: 1.4612603187561035\n",
      "Loss: 1.468651533126831\n",
      "Loss: 1.4688076972961426\n",
      "Loss: 1.4633917808532715\n",
      "Loss: 1.4630134105682373\n",
      "Loss: 1.4619660377502441\n",
      "Loss: 1.4613313674926758\n",
      "Loss: 1.467590093612671\n",
      "Loss: 1.4611738920211792\n",
      "Loss: 1.464718222618103\n",
      "Loss: 1.461173176765442\n",
      "Loss: 1.4612782001495361\n",
      "Loss: 1.5185000896453857\n",
      "Loss: 1.461211919784546\n",
      "Loss: 1.5071359872817993\n",
      "Loss: 1.4627978801727295\n",
      "Loss: 1.4621020555496216\n",
      "Loss: 1.463412880897522\n",
      "Loss: 1.4666361808776855\n",
      "Loss: 1.4612549543380737\n",
      "Loss: 1.461798071861267\n",
      "Loss: 1.482903242111206\n",
      "Loss: 1.4717552661895752\n",
      "Loss: 1.4625320434570312\n",
      "Loss: 1.4871536493301392\n",
      "Loss: 1.4613873958587646\n",
      "Loss: 1.4612516164779663\n",
      "Loss: 1.7019762992858887\n",
      "Loss: 1.4902260303497314\n",
      "Loss: 1.4613616466522217\n",
      "Loss: 1.524059534072876\n",
      "Loss: 1.5154095888137817\n",
      "Loss: 1.4785884618759155\n",
      "Loss: 1.4643142223358154\n",
      "Loss: 1.4675542116165161\n",
      "Loss: 1.464552640914917\n",
      "Loss: 1.6784995794296265\n",
      "Loss: 1.4651116132736206\n",
      "Loss: 1.4899970293045044\n",
      "Loss: 1.463525414466858\n",
      "Loss: 1.4654982089996338\n",
      "Loss: 1.4612219333648682\n",
      "Loss: 1.5254807472229004\n",
      "Loss: 1.4668165445327759\n",
      "Loss: 1.5229973793029785\n",
      "Loss: 1.4622317552566528\n",
      "Loss: 1.4872300624847412\n",
      "Loss: 1.4611729383468628\n",
      "Loss: 1.475287675857544\n",
      "Loss: 1.5421143770217896\n",
      "Loss: 1.665069818496704\n",
      "Loss: 1.4811630249023438\n",
      "Loss: 1.5099458694458008\n",
      "Loss: 1.4613205194473267\n",
      "Loss: 1.6542251110076904\n",
      "Loss: 1.470521092414856\n",
      "Loss: 1.676734447479248\n",
      "Loss: 1.4658515453338623\n",
      "Loss: 1.4620473384857178\n",
      "Loss: 1.4612524509429932\n",
      "Loss: 1.4614421129226685\n",
      "Loss: 1.4615364074707031\n",
      "Loss: 1.7100207805633545\n",
      "Loss: 1.4617640972137451\n",
      "Loss: 1.7072709798812866\n",
      "Loss: 1.461761236190796\n",
      "Loss: 1.463158369064331\n",
      "Loss: 1.5341905355453491\n",
      "Loss: 1.462172508239746\n",
      "Loss: 1.718587875366211\n",
      "Loss: 1.5328091382980347\n",
      "Loss: 1.4620118141174316\n",
      "Loss: 1.7060232162475586\n",
      "Loss: 1.461876630783081\n",
      "Loss: 1.711363673210144\n",
      "Loss: 1.4674041271209717\n",
      "Loss: 1.8276869058609009\n",
      "Loss: 1.4617820978164673\n",
      "Loss: 1.4713613986968994\n",
      "Loss: 1.5374799966812134\n",
      "Loss: 1.4623920917510986\n",
      "Loss: 1.5013597011566162\n",
      "Loss: 1.5594877004623413\n",
      "Loss: 1.4836724996566772\n",
      "Loss: 1.4625903367996216\n",
      "Loss: 1.7134406566619873\n",
      "Loss: 1.4731194972991943\n",
      "Loss: 1.4669522047042847\n",
      "Loss: 1.461698055267334\n",
      "Loss: 1.5495738983154297\n",
      "Loss: 1.8225071430206299\n",
      "Loss: 1.7036340236663818\n",
      "Loss: 1.4611799716949463\n",
      "Loss: 1.4673025608062744\n",
      "Loss: 1.4779026508331299\n",
      "Loss: 1.8177313804626465\n",
      "Loss: 1.4670873880386353\n",
      "Loss: 1.6734622716903687\n",
      "Loss: 1.748198390007019\n",
      "Loss: 1.944042682647705\n",
      "Loss: 1.4860466718673706\n",
      "Loss: 1.5909919738769531\n",
      "Loss: 2.003660202026367\n",
      "Loss: 1.8105592727661133\n",
      "Loss: 1.7790418863296509\n",
      "Loss: 1.5022016763687134\n",
      "Loss: 1.4705075025558472\n",
      "Loss: 1.6635692119598389\n",
      "Loss: 2.015204906463623\n",
      "Loss: 1.4830167293548584\n",
      "Loss: 1.9391491413116455\n",
      "Loss: 1.5221741199493408\n",
      "Loss: 1.463220477104187\n",
      "Loss: 1.4683254957199097\n",
      "Loss: 1.6813832521438599\n",
      "Loss: 1.4614521265029907\n",
      "Loss: 1.463568925857544\n",
      "Loss: 1.4617750644683838\n",
      "Loss: 1.941697120666504\n",
      "Loss: 1.473419189453125\n",
      "Loss: 1.5003340244293213\n",
      "Loss: 1.4616342782974243\n",
      "Loss: 1.4649661779403687\n",
      "Loss: 1.4926503896713257\n",
      "Loss: 1.6771180629730225\n",
      "Loss: 1.7102601528167725\n",
      "Loss: 1.6206245422363281\n",
      "Loss: 1.5015562772750854\n",
      "Loss: 1.468942642211914\n",
      "Loss: 1.5558843612670898\n",
      "Loss: 1.8483330011367798\n",
      "Loss: 1.493978500366211\n",
      "Loss: 1.7103253602981567\n",
      "Loss: 1.4631984233856201\n",
      "Loss: 1.6779106855392456\n",
      "Loss: 1.5657278299331665\n",
      "Loss: 1.6962268352508545\n",
      "Loss: 1.4776031970977783\n",
      "Loss: 1.755053162574768\n",
      "Loss: 1.8920649290084839\n",
      "Loss: 1.461228609085083\n",
      "Loss: 1.5382885932922363\n",
      "Loss: 1.7241106033325195\n",
      "Loss: 1.4635770320892334\n",
      "Loss: 1.4634472131729126\n",
      "Loss: 1.5677995681762695\n",
      "Loss: 1.4850387573242188\n",
      "Loss: 1.6773977279663086\n",
      "Loss: 1.47567880153656\n",
      "Loss: 1.4656305313110352\n",
      "Loss: 1.473929524421692\n",
      "Loss: 2.049116611480713\n",
      "Loss: 1.7173360586166382\n",
      "Loss: 1.4620307683944702\n",
      "Loss: 1.7008191347122192\n",
      "Loss: 1.6016308069229126\n",
      "Loss: 1.464982509613037\n",
      "Loss: 1.4776519536972046\n",
      "Loss: 1.7100166082382202\n",
      "Loss: 1.684942603111267\n",
      "Loss: 1.4640079736709595\n",
      "Loss: 1.9457221031188965\n",
      "Loss: 1.5465558767318726\n",
      "Loss: 1.4638493061065674\n",
      "Loss: 1.464707374572754\n",
      "Loss: 1.504431128501892\n",
      "=========\n",
      "The average loss is: 1.5467981100082397\n",
      "The final loss is: 1.504431128501892\n",
      "The final accuracy is: 92.30000000000001%\n"
     ]
    }
   ],
   "source": [
    "test(model, mnist_testset, loss_function, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and predict (0~9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- [How to use Jupyter notebooks in a conda environment?](https://stackoverflow.com/questions/58068818/how-to-use-jupyter-notebooks-in-a-conda-environment)\n",
    "\n",
    "- [What is the difference between .data.size() and .size() in PyTorch?](https://stackoverflow.com/questions/68275587/what-is-the-difference-between-data-size-and-size-in-pytorch)\n",
    "\n",
    "- [Pytorch：torch.flatten()与torch.nn.Flatten()](https://blog.csdn.net/Super_user_and_woner/article/details/120782656)\n",
    "\n",
    "- [Pytorch中dataloader之enumerate与iter，tqdm](https://blog.csdn.net/jzwong/article/details/103777338)\n",
    "\n",
    "- [【深度学习】LeNet网络架构](https://blog.csdn.net/qq_43466788/article/details/133121184?spm=1001.2101.3001.6650.17&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBLOGTAG%7Edefault-17-133121184-blog-130553358.235%5Ev38%5Epc_relevant_anti_vip_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBLOGTAG%7Edefault-17-133121184-blog-130553358.235%5Ev38%5Epc_relevant_anti_vip_base&utm_relevant_index=23)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
